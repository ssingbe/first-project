{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3단계 여러페이지 크롤링하는 법 실습 + 데이터를 리스트 형태로 저장\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for i in range(1,5):\n",
    "    response = requests.get(\"https://startcoding.pythonanywhere.com/basic?page{i}\")#response 응답객체 데이터+명령어를 모두가지는 자료형\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    items = soup.select(\".product\")\n",
    "\n",
    "    for item in items:\n",
    "        category = item.select_one(\".product-category\").text\n",
    "        name = item.select_one(\".product-name\").text\n",
    "        link = item.select_one(\".product-name > a\").attrs['href']\n",
    "        price = item.select_one(\".product-price\").text.split('원')[0].replace(',','') #strip 앞뒤 공백제거, replace는 원하는값 대체\n",
    "        print(category,name,link,price)\n",
    "        data.append([category,name,link,price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 프레임 만들기\n",
    "df = pd.DataFrame(data, columns=['카테고리','상품명','링크','가격'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#엑셀 저장\n",
    "df.to_excel('result.xlsx', index = False) # index = false는 첫열의 자동 번호부여를 지우는 용도임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1단계 한개의 상품 크롤링 실습(CES 사이트)- 추가 Data 수집 확인\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.ces.tech/innovation-awards/2025/sl-22-high-resolution-spad-sensor-for-lidar/\")\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "description = soup.select_one(\".rich-text\").text.strip() if soup.select_one(\".rich-text\") else \"Description not found\"\n",
    "prizes = soup.select_one(\".mt-8.first\\:mt-0.f-body-2\").text.strip() if soup.select_one(\".mt-8.first\\:mt-0.f-body-2\") else \"Prizes not found\"\n",
    "company_des = soup.select_one(\".mt-16.text-secondary.f-body-2\").text.strip() if soup.select_one(\".mt-16.text-secondary.f-body-2\") else \"Company_des not found\"\n",
    "booths = soup.select_one(\".mt-4.first\\:mt-0\").text.strip() if soup.select_one(\".mt-4.first\\:mt-0\") else \"Booths not found\"\n",
    "booths_tag = soup.select_one(\".mt-4.first\\:mt-0 > a\")\n",
    "booths_link = booths_tag.attrs['href'] if booths_tag and 'href' in booths_tag.attrs else \"Booths_link not found\"\n",
    "image_link = soup.select_one(\".flex.justify-center.items-center.h-full.aspect-16\\/9 img\")\n",
    "\n",
    "print(image_link) #description, prizes, company_des, booths, booths_link, image_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3단계 여러페이지 크롤링하는 법 실습(CES)+이미지 저장\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image  # 이미지 삽입을 위한 모듈\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# 안전한 파일 이름으로 변환하는 함수\n",
    "def sanitize_filename(filename):\n",
    "    # Windows에서 허용되지 않는 문자를 다른 문자로 대체\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "\n",
    "# 이미지 다운로드 및 변환 함수\n",
    "def download_and_convert_image(image_url, save_path):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            # 원본 이미지 저장\n",
    "            temp_path = \"temp.webp\"  # 임시로 .webp로 저장\n",
    "            with open(temp_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            # .webp를 .jpg로 변환\n",
    "            with PILImage.open(temp_path) as img:\n",
    "                img.convert(\"RGB\").save(save_path, \"JPEG\")  # .jpg 형식으로 저장\n",
    "\n",
    "            # 임시 파일 삭제\n",
    "            os.remove(temp_path)\n",
    "            return save_path\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading or converting image {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Booths 번호 추출 및 포맷 함수\n",
    "def extract_booths_number_and_format(booths_link):\n",
    "    try:\n",
    "        match = re.search(r'booth~([A-Za-z0-9%]+)', booths_link)\n",
    "        if match:\n",
    "            extracted_text = match.group(1)\n",
    "            return extracted_text.replace('%', ' ')  # \"%\"를 빈칸으로 대체\n",
    "        else:\n",
    "            return \"Booth number not found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting booth number: {str(e)}\"\n",
    "\n",
    "#제품 링크에서 추가 정보를 가져오는 함수\n",
    "def get_additional_info(product_link):\n",
    "    try:\n",
    "        response = requests.get(product_link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 제품 설명 (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            description = soup.select_one(\".rich-text\")\n",
    "            description = description.text.strip() if description else \"Description not found\"\n",
    "            \n",
    "            # 제품 사양 (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            prizes = soup.select_one(\".mt-8.first\\:mt-0.f-body-2\")\n",
    "            prizes = prizes.text.strip() if prizes else \"Prizes not found\"\n",
    "\n",
    "            # 기업설명 (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            company_des = soup.select_one(\".mt-16.text-secondary.f-body-2\")\n",
    "            company_des = company_des.text.strip() if company_des else \"Company_des not found\"\n",
    "\n",
    "            # Booths (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            booths = soup.select_one(\".mt-4.first\\:mt-0\")\n",
    "            booths = booths.text.strip() if booths else \"Booths not found\"\n",
    "\n",
    "            # Booths_link (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            booths_tag = soup.select_one(\".mt-4.first\\:mt-0 > a\")\n",
    "            booths_link = booths_tag.attrs['href'] if booths_tag and 'href' in booths_tag.attrs else \"Booths_link not found\"\n",
    "\n",
    "            # \n",
    "            image_tag = soup.select_one(\".relative.flex.flex-col.h-full.p-16.md\\:pb-24.rounded-lg.bg-layer.text-primary.transition.group\\/card img\")\n",
    "            image_url = image_tag.attrs.get('src', \"Image not found\") if image_tag else \"Image not found\"\n",
    "\n",
    "            # 상대 경로일 경우 절대 경로로 변환\n",
    "            if image_url != \"Image not found\" and not image_url.startswith('http'):\n",
    "                image_url = f\"https://www.ces.tech{image_url}\"\n",
    "\n",
    "            return description, prizes, company_des, booths, booths_link, image_url\n",
    "        else:\n",
    "            return \"Description not found\", \"Prizes not found\", \"company_des not found\", \"Booths not found\", \"Booths_link not found\", \"Image not found\"\n",
    "    except Exception as e:\n",
    "        return \"Error retrieving info\", str(e)\n",
    "    \n",
    "# 이미지 저장 경로가 없으면 디렉토리 생성하는 함수\n",
    "def create_directory_for_image(image_filename):\n",
    "    # 이미지 파일 경로에 해당하는 디렉토리가 없으면 생성\n",
    "    directory = os.path.dirname(image_filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "#모든 페이지에서 코롤링한 데이터를 저장할 리스트\n",
    "all_data = []\n",
    "images_dir = \"images\"  # 이미지 저장 폴더\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# Excel Workbook 생성\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"CES Innovation Awards\"\n",
    "columns = [\"Company\", \"Product\", \"Category\", \"Link\", \"Description\", \"Prizes\", \"Company Description\", \"Booths\", \"Booths Link\", \"Booth Number\", \"Image\"]\n",
    "ws.append(columns)\n",
    "\n",
    "for i in range(1,24):\n",
    "    try:\n",
    "        response = requests.get(f\"https://www.ces.tech/innovation-awards/?page={i}\")#response 응답객체 데이터+명령어를 모두가지는 자료형\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        items = soup.select(\".relative.flex.flex-col.h-full.p-16.md\\:pb-24.rounded-lg.bg-layer.text-primary.transition.group\\/card\")\n",
    "\n",
    "        for item in items:\n",
    "            try:\n",
    "                company = item.select_one(\".outline-none.f-heading-4\").text.strip()\n",
    "                product = item.select_one(\".mt-4.f-body-3\").text.strip()\n",
    "                category = item.select_one(\".relative.flex.flex-wrap.gap-y-8.gap-x-4.mt-auto.pt-space-6.z-2\").text.strip() #strip 앞뒤 공백제거, replace는 원하는값 대체\n",
    "                link = item.select_one(\"a\").attrs['href']\n",
    "\n",
    "                # 제품 링크에서 추가 정보 가져오기\n",
    "                full_link = f\"https://www.ces.tech{link}\" #상대 URL을 절대 URL로 변환\n",
    "\n",
    "                # 제품 링크에서 추가 정보 가져오기\n",
    "                description, prizes, company_des, booths, booths_link, image_url = get_additional_info(full_link)  # 추가 정보 가져오기\n",
    "\n",
    "                # Booths 번호 추출 및 포맷\n",
    "                booths_number = extract_booths_number_and_format(booths_link)\n",
    "\n",
    "                # 이미지 URL을 안전한 파일 이름으로 처리\n",
    "                if image_url != \"Image not found\":\n",
    "                    # 이미지 파일 이름을 안전하게 처리\n",
    "                    image_filename = os.path.join(images_dir, sanitize_filename(f\"{company}_{product}.jpg\"))\n",
    "                    # 디렉토리가 없으면 생성\n",
    "                    create_directory_for_image(image_filename)\n",
    "                    # 이미지 다운로드 및 저장\n",
    "                    image_path = download_and_convert_image(image_url, image_filename)\n",
    "                else:\n",
    "                    image_path = None\n",
    "\n",
    "                # Excel에 추가할 데이터\n",
    "                row = [company, product, category, full_link, description, prizes, company_des, booths, booths_link, booths_number]\n",
    "\n",
    "                # Excel에 데이터 추가\n",
    "                ws.append(row)\n",
    "\n",
    "                # 이미지가 있으면 Excel에 삽입\n",
    "                if image_path:\n",
    "                    current_row = ws.max_row  # 현재 행 번호 추출\n",
    "                    img = Image(image_path)\n",
    "                    img.width = 100  # 이미지 크기 조정\n",
    "                    img.height = 100\n",
    "                    img.anchor = f\"K{current_row}\"  # 이미지 삽입할 셀 지정\n",
    "                    ws.add_image(img)\n",
    "\n",
    "                # 요청 간에 시간 지연 추가 (서버 과부하 방지)\n",
    "                time.sleep(0.1)  # 0.1초 대기\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing item on page {i}: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing page {i}: {e}\")\n",
    "\n",
    "# Excel 파일 저장\n",
    "output_file = \"CES_Innovation_Awards_with_Images.xlsx\"\n",
    "wb.save(output_file)\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image  # 이미지 삽입을 위한 모듈\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# 안전한 파일 이름으로 변환하는 함수\n",
    "def sanitize_filename(filename):\n",
    "    # Windows에서 허용되지 않는 문자를 다른 문자로 대체\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "\n",
    "# 이미지 다운로드 및 변환 함수\n",
    "def download_and_convert_image(image_url, save_path):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            # 원본 이미지 저장\n",
    "            temp_path = \"temp.webp\"  # 임시로 .webp로 저장\n",
    "            with open(temp_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            # .webp를 .jpg로 변환\n",
    "            with PILImage.open(temp_path) as img:\n",
    "                img.convert(\"RGB\").save(save_path, \"JPEG\")  # .jpg 형식으로 저장\n",
    "\n",
    "            # 임시 파일 삭제\n",
    "            os.remove(temp_path)\n",
    "            return save_path\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading or converting image {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Booths 번호 추출 및 포맷 함수\n",
    "def extract_booths_number_and_format(booths_link):\n",
    "    try:\n",
    "        match = re.search(r'booth~([A-Za-z0-9%]+)', booths_link)\n",
    "        if match:\n",
    "            extracted_text = match.group(1)\n",
    "            return extracted_text.replace('%', ' ')  # \"%\"를 빈칸으로 대체\n",
    "        else:\n",
    "            return \"Booth number not found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting booth number: {str(e)}\"\n",
    "\n",
    "# 제품 링크에서 추가 정보를 가져오는 함수\n",
    "def get_additional_info(product_link):\n",
    "    try:\n",
    "        response = requests.get(product_link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 제품 설명 (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            description = soup.select_one(\".rich-text\")\n",
    "            description = description.text.strip() if description else \"Description not found\"\n",
    "            \n",
    "            # 제품 사양 (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            prizes = soup.select_one(\".mt-8.first\\:mt-0.f-body-2\")\n",
    "            prizes = prizes.text.strip() if prizes else \"Prizes not found\"\n",
    "\n",
    "            # 기업설명 (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            company_des = soup.select_one(\".mt-16.text-secondary.f-body-2\")\n",
    "            company_des = company_des.text.strip() if company_des else \"Company_des not found\"\n",
    "\n",
    "            # Booths (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            booths = soup.select_one(\".mt-4.first\\:mt-0\")\n",
    "            booths = booths.text.strip() if booths else \"Booths not found\"\n",
    "\n",
    "            # Booths_link (HTML 구조에 맞게 CSS 선택자 수정 필요)\n",
    "            booths_tag = soup.select_one(\".mt-4.first\\:mt-0 > a\")\n",
    "            booths_link = booths_tag.attrs['href'] if booths_tag and 'href' in booths_tag.attrs else \"Booths_link not found\"\n",
    "\n",
    "            # 이미지 URL 추출\n",
    "            image_tag = soup.select_one(\".relative.flex.flex-col.h-full.p-16.md\\:pb-24.rounded-lg.bg-layer.text-primary.transition.group\\/card img\")\n",
    "            image_url = image_tag.attrs.get('src', \"Image not found\") if image_tag else \"Image not found\"\n",
    "\n",
    "            # 상대 경로일 경우 절대 경로로 변환\n",
    "            if image_url != \"Image not found\" and not image_url.startswith('http'):\n",
    "                image_url = f\"https://www.ces.tech{image_url}\"\n",
    "\n",
    "            return description, prizes, company_des, booths, booths_link, image_url\n",
    "        else:\n",
    "            return \"Description not found\", \"Prizes not found\", \"company_des not found\", \"Booths not found\", \"Booths_link not found\", \"Image not found\"\n",
    "    except Exception as e:\n",
    "        return \"Error retrieving info\", str(e)\n",
    "\n",
    "# 이미지 URL 추출 함수\n",
    "def get_image_url_from_item(item):\n",
    "    image_tag = item.select_one(\".relative.flex.flex-col.h-full.p-16.md\\:pb-24.rounded-lg.bg-layer.text-primary.transition.group\\/card img\")\n",
    "    return image_tag.attrs.get('src', \"Image not found\") if image_tag else \"Image not found\"\n",
    "\n",
    "# 이미지 저장 경로가 없으면 디렉토리 생성하는 함수\n",
    "def create_directory_for_image(image_filename):\n",
    "    # 이미지 파일 경로에 해당하는 디렉토리가 없으면 생성\n",
    "    directory = os.path.dirname(image_filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# 모든 페이지에서 크롤링한 데이터를 저장할 리스트\n",
    "all_data = []\n",
    "images_dir = \"images\"  # 이미지 저장 폴더\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# Excel Workbook 생성\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"CES Innovation Awards\"\n",
    "columns = [\"Company\", \"Product\", \"Category\", \"Link\", \"Description\", \"Prizes\", \"Company Description\", \"Booths\", \"Booths Link\", \"Booth Number\", \"Image\"]\n",
    "ws.append(columns)\n",
    "\n",
    "for i in range(1, 24):\n",
    "    try:\n",
    "        response = requests.get(f\"https://www.ces.tech/innovation-awards/?page={i}\")  # response 응답객체 데이터+명령어를 모두 가짐\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        items = soup.select(\".relative.flex.flex-col.h-full.p-16.md\\:pb-24.rounded-lg.bg-layer.text-primary.transition.group\\/card\")\n",
    "\n",
    "        for item in items:\n",
    "            try:\n",
    "                # 기본 정보 추출\n",
    "                company = item.select_one(\".outline-none.f-heading-4\").text.strip()\n",
    "                product = item.select_one(\".mt-4.f-body-3\").text.strip()\n",
    "                category = item.select_one(\".relative.flex.flex-wrap.gap-y-8.gap-x-4.mt-auto.pt-space-6.z-2\").text.strip()\n",
    "                link = item.select_one(\"a\").attrs['href']\n",
    "                full_link = f\"https://www.ces.tech{link}\"\n",
    "\n",
    "                # 이미지 URL 추출\n",
    "                image_url = get_image_url_from_item(item)\n",
    "\n",
    "                # 제품 링크에서 추가 정보 가져오기\n",
    "                description, prizes, company_des, booths, booths_link, image_url = get_additional_info(full_link)\n",
    "\n",
    "                # Booths 번호 추출 및 포맷\n",
    "                booths_number = extract_booths_number_and_format(booths_link)\n",
    "\n",
    "                # 이미지 다운로드 및 저장\n",
    "                image_path = None\n",
    "                if image_url != \"Image not found\":\n",
    "                    image_filename = os.path.join(images_dir, sanitize_filename(f\"{company}_{product}.jpg\"))\n",
    "                    create_directory_for_image(image_filename)\n",
    "                    image_path = download_and_convert_image(image_url, image_filename)\n",
    "\n",
    "                # Excel에 추가할 데이터\n",
    "                row = [company, product, category, full_link, description, prizes, company_des, booths, booths_link, booths_number]\n",
    "                ws.append(row)\n",
    "\n",
    "                # 이미지가 있으면 Excel에 삽입\n",
    "                if image_path:\n",
    "                    img = Image(image_path)\n",
    "                    img.width = 100\n",
    "                    img.height = 100\n",
    "                    img.anchor = f\"K{ws.max_row}\"\n",
    "                    ws.add_image(img)\n",
    "\n",
    "                # 요청 간에 시간 지연 추가 (서버 과부하 방지)\n",
    "                time.sleep(0.1)  # 0.1초 대기\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing item on page {i}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing page {i}: {e}\")\n",
    "\n",
    "# Excel 파일 저장\n",
    "output_file = \"CES_Innovation_Awards_with_Images.xlsx\"\n",
    "wb.save(output_file)\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    \"\"\" 폴더가 존재하지 않으면 생성 \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"폴더 생성됨: {folder_path}\")\n",
    "\n",
    "def create_excel_if_not_exists(file_path):\n",
    "    \"\"\" 엑셀 파일이 없으면 생성 \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        df = pd.DataFrame(columns=['File Name', 'URL', 'Status'])\n",
    "        df.to_excel(file_path, index=False, engine='openpyxl')  # 엔진 명시\n",
    "        print(f\"엑셀 파일 생성됨: {file_path}\")\n",
    "\n",
    "def update_excel(file_path, file_name, url, status):\n",
    "    \"\"\" 엑셀 파일에 다운로드 상태 업데이트 \"\"\"\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')  # 엔진 명시\n",
    "    new_entry = pd.DataFrame([[file_name, url, status]], columns=['File Name', 'URL', 'Status'])\n",
    "    df = pd.concat([df, new_entry], ignore_index=True)\n",
    "    df.to_excel(file_path, index=False, engine='openpyxl')  # 엔진 명시\n",
    "    print(f\"{file_name} 상태 업데이트됨: {status}\")\n",
    "\n",
    "def get_pdf_links(base_url_template, issue_range, page_range, save_folder, excel_file):\n",
    "    pdf_links = []\n",
    "\n",
    "    # 폴더 생성\n",
    "    create_folder_if_not_exists(save_folder)\n",
    "\n",
    "    # 엑셀 파일 준비\n",
    "    create_excel_if_not_exists(excel_file)\n",
    "\n",
    "    # 세션 생성\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Referer': 'https://www.jstage.jst.go.jp/',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Connection': 'keep-alive'  # 연결 유지\n",
    "        })\n",
    "\n",
    "        # 파일 목록 가져오기\n",
    "        for issue in issue_range:\n",
    "            for page in page_range:\n",
    "                url = base_url_template.format(issue=issue, page=page)\n",
    "                print(f\"접속 중: {url}\")\n",
    "\n",
    "                # 페이지 요청\n",
    "                response = make_request(session, url)\n",
    "                if response is None:\n",
    "                    continue\n",
    "\n",
    "                # BeautifulSoup으로 HTML 파싱\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # 각 li 태그 내에서 PDF 링크와 파일명 추출\n",
    "                for li in soup.find_all('li'):\n",
    "                    # 파일명: div class=\"searchlist-title\"\n",
    "                    title_div = li.find('div', class_='searchlist-title')\n",
    "                    if title_div:\n",
    "                        file_name = title_div.get_text(strip=True)  # 파일명 추출\n",
    "\n",
    "                        # 다운로드 링크: div class=\"lft\" -> span 하위 a 태그\n",
    "                        lft_div = li.find('div', class_='lft')\n",
    "                        if lft_div:\n",
    "                            a_tag = lft_div.find('span').find('a', href=True)\n",
    "                            if a_tag and 'pdf' in a_tag['href'].lower():  # href에 pdf 포함 확인\n",
    "                                pdf_link = a_tag['href']\n",
    "                                full_url = pdf_link if pdf_link.startswith('http') else \"https://www.jstage.jst.go.jp\" + pdf_link\n",
    "                                \n",
    "                                # PDF 파일 다운로드 상태 기록\n",
    "                                update_excel(excel_file, file_name, full_url, 'Pending')  # 대기 중 상태로 엑셀에 기록\n",
    "\n",
    "                                pdf_links.append((full_url, file_name, save_folder))\n",
    "\n",
    "    return pdf_links\n",
    "\n",
    "def make_request(session, url, retries=5, delay=10):\n",
    "    \"\"\" 요청을 보내고 실패시 재시도하는 함수 \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"시도 중 {attempt + 1}/{retries}: {url}\")\n",
    "            response = session.get(url)\n",
    "            response.raise_for_status()  # 상태 코드가 200이 아니면 예외 발생\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"요청 실패: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"{delay}초 후 재시도...\")\n",
    "                time.sleep(delay)  # 재시도 전에 잠시 대기\n",
    "            else:\n",
    "                print(\"최대 재시도 횟수 초과\")\n",
    "    return None\n",
    "\n",
    "def download_pdf(pdf_url, file_name, save_folder, excel_file):\n",
    "    try:\n",
    "        # PDF 파일명에 .pdf 확장자 추가\n",
    "        file_name = file_name + \".pdf\"\n",
    "        save_path = os.path.join(save_folder, file_name)\n",
    "\n",
    "        # PDF 다운로드 요청\n",
    "        if os.path.exists(save_path):  # 이미 존재하면 다운로드 건너뛰기\n",
    "            print(f\"{file_name} 이미 존재, 건너뜁니다.\")\n",
    "            update_excel(excel_file, file_name, pdf_url, 'Already Downloaded')\n",
    "            return\n",
    "\n",
    "        print(f\"다운로드 시작: {pdf_url}\")\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as pdf_file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    pdf_file.write(chunk)\n",
    "            print(f\"다운로드 완료: {save_path}\")\n",
    "            update_excel(excel_file, file_name, pdf_url, 'Downloaded')  # 다운로드 완료 상태로 업데이트\n",
    "        else:\n",
    "            print(f\"다운로드 실패: {response.status_code}\")\n",
    "            update_excel(excel_file, file_name, pdf_url, 'Failed')  # 실패 상태로 업데이트\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        update_excel(excel_file, file_name, pdf_url, 'Failed')  # 실패 상태로 업데이트\n",
    "\n",
    "def download_all_pdfs_in_parallel(pdf_links, excel_file):\n",
    "    \"\"\" 병렬로 PDF 다운로드 \"\"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(download_pdf, pdf_url, file_name, save_folder, excel_file) for pdf_url, file_name, save_folder in pdf_links]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "# 실행 예제\n",
    "base_url_template = \"https://www.jstage.jst.go.jp/browse/denkiseiko/{issue}/{page}/_contents/-char/en\"\n",
    "issue_range = range(77, 80)  # 발행수 1~2\n",
    "page_range = range(1, 5)   # 페이지 수 1~2\n",
    "save_folder = \"downloaded_pdfs\"  # PDF를 저장할 폴더\n",
    "excel_file = \"download_status.xlsx\"  # 엑셀 파일 경로\n",
    "\n",
    "# PDF 링크 가져오기\n",
    "pdf_links = get_pdf_links(base_url_template, issue_range, page_range, save_folder, excel_file)\n",
    "\n",
    "# 병렬로 PDF 다운로드\n",
    "download_all_pdfs_in_parallel(pdf_links, excel_file)\n",
    "\n",
    "print(\"모든 PDF 다운로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# PDF 링크 및 추가 데이터 수집 함수 (page_range 제거, 페이지 자동 탐색)\n",
    "def get_pdf_links(base_url_template, issue_range, excel_file, save_interval=10):\n",
    "    all_data = []  # 모든 데이터를 저장할 리스트\n",
    "    count = 0  # 저장 주기 카운터\n",
    "\n",
    "    # 세션 생성\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update({\n",
    "            'User-Agent': get_random_user_agent(),\n",
    "            'Referer': 'https://www.jstage.jst.go.jp/',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'\n",
    "        })\n",
    "\n",
    "        # issue별 페이지 번호 탐색\n",
    "        for issue in issue_range:\n",
    "            page = 1\n",
    "            while True:\n",
    "                url = base_url_template.format(issue=issue, page=page)\n",
    "                print(f\"접속 중: {url}\")\n",
    "\n",
    "                # 페이지 요청\n",
    "                response = make_request(session, url)\n",
    "                if response is None:\n",
    "                    break  # 페이지 로드 실패 시 종료\n",
    "\n",
    "                # BeautifulSoup으로 HTML 파싱\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # 페이지에 내용이 없으면 종료 (더 이상 페이지가 없다는 의미)\n",
    "                if not soup.find_all('li'):\n",
    "                    print(f\"페이지 {page}에는 더 이상 데이터가 없습니다. 탐색 종료.\")\n",
    "                    break\n",
    "\n",
    "                # 각 li 태그 내에서 PDF 링크, 파일명, 저자명, 추가정보 추출\n",
    "                for li in soup.find_all('li'):\n",
    "                    # 파일명: div class=\"searchlist-title\"\n",
    "                    title_div = li.find('div', class_='searchlist-title')\n",
    "                    if title_div:\n",
    "                        file_name = title_div.get_text(strip=True)\n",
    "\n",
    "                        # 저자명: div class=\"searchlist-authortags customTooltip\" -> title 속성\n",
    "                        author_div = li.find('div', class_='searchlist-authortags customTooltip')\n",
    "                        authors = author_div['title'] if author_div and 'title' in author_div.attrs else \"N/A\"\n",
    "\n",
    "                        # 추가정보: div class=\"searchlist-additional-info\" -> <br> 기준으로 나누기\n",
    "                        additional_info_div = li.find('div', class_='searchlist-additional-info')\n",
    "                        additional_info_parts = []\n",
    "                        if additional_info_div:\n",
    "                            additional_info_parts = [part.strip() for part in additional_info_div.get_text(separator=\"\\n\").split(\"\\n\") if part.strip()]\n",
    "                        \n",
    "                        # 다운로드 링크: div class=\"lft\" -> span 하위 a 태그\n",
    "                        lft_div = li.find('div', class_='lft')\n",
    "                        if lft_div:\n",
    "                            a_tag = lft_div.find('span').find('a', href=True)\n",
    "                            if a_tag and 'pdf' in a_tag['href'].lower():\n",
    "                                pdf_link = a_tag['href']\n",
    "                                full_url = pdf_link if pdf_link.startswith('http') else \"https://www.jstage.jst.go.jp\" + pdf_link\n",
    "\n",
    "                                # 데이터 저장\n",
    "                                data_entry = {\n",
    "                                    'Issue': issue,\n",
    "                                    'Page': page,\n",
    "                                    'FileName': file_name,\n",
    "                                    'Author': authors,\n",
    "                                    'PDF_Link': full_url\n",
    "                                }\n",
    "                                # 추가정보를 각 열로 추가\n",
    "                                for idx, info in enumerate(additional_info_parts):\n",
    "                                    data_entry[f\"Additional_Info_{idx + 1}\"] = info\n",
    "                                \n",
    "                                all_data.append(data_entry)\n",
    "                                count += 1\n",
    "\n",
    "                                # 주기적으로 저장\n",
    "                                if count % save_interval == 0:\n",
    "                                    save_to_excel(all_data, excel_file)\n",
    "                                    print(f\"{count}개 항목 저장됨.\")\n",
    "                                    all_data.clear()  # 메모리 초기화\n",
    "\n",
    "                page += 1  # 다음 페이지로 이동\n",
    "                time.sleep(random.randint(2, 5))  # 랜덤 딜레이 적용\n",
    "\n",
    "    # 남은 데이터 저장\n",
    "    if all_data:\n",
    "        save_to_excel(all_data, excel_file)\n",
    "        print(f\"마지막 데이터 {len(all_data)}개 저장 완료.\")\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def make_request(session, url, retries=5, delay=10):\n",
    "    \"\"\" 요청을 보내고 실패시 재시도하는 함수 \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"시도 중 {attempt + 1}/{retries}: {url}\")\n",
    "            response = session.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"요청 실패: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                sleep_time = random.randint(2, delay)\n",
    "                print(f\"{sleep_time}초 후 재시도...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(\"최대 재시도 횟수 초과\")\n",
    "    return None\n",
    "\n",
    "def save_to_excel(data, excel_file):\n",
    "    \"\"\" 데이터를 엑셀 파일에 저장하는 함수 \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    if not data:\n",
    "        print(\"저장할 데이터가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 엑셀 파일이 이미 존재하면 기존 데이터와 병합\n",
    "    if os.path.exists(excel_file):\n",
    "        existing_df = pd.read_excel(excel_file)\n",
    "        df = pd.concat([existing_df, df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    df.to_excel(excel_file, index=False)\n",
    "    print(f\"데이터 저장 완료: {excel_file}\")\n",
    "\n",
    "# 실행 예제\n",
    "base_url_template = \"https://www.jstage.jst.go.jp/browse/denkiseiko/{issue}/{page}/_contents/-char/en\"\n",
    "issue_range = range(1, 39)  # 발행수 범위\n",
    "excel_file = \"pdf_links_with_authors3.xlsx\"\n",
    "\n",
    "get_pdf_links(base_url_template, issue_range, excel_file)\n",
    "print(\"모든 데이터 수집 완료.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
