<v ->Ladies and gentlemen, welcome to CES 2025.</v>
I'm Gary Shapiro, CEO,
and Vice Chairman of the Consumer Technology Association,
the producer of CES.
And I am so thrilled to kick off this show with a keynote
by one of the most consequential companies in the world,
NVIDIA exemplifies the cutting edge innovation we celebrate
at CES and Founder and CEO Jensen Huang is a true visionary
demonstrating the power of ideas,
technology, and conviction to drive innovation
and reshape our industry and our society.
I always like to say
that if I listened a little bit more closely
the last time Jensen spoke at a CTA event,
I could have retired already.
But over the past three decades, he has established NVIDIA
as a force driving change across the globe
in industries ranging from healthcare
to automotive and entertainment.
Today, NVIDIA is pioneering breakthroughs in AI
and accelerated computing that touch nearly every person
and every business.
Thanks to his leadership,
NVIDIA's innovations enable advanced chatbots, robots,
software defined vehicles, huge virtual worlds,
hyper synchronized factory floors, and so much more.
Huang has been named the world's best CEO by Fortune
and the Economist, as well as one
of Time magazine's 100 most influential people in the world.
But you know, the fact is, like for all of us in this room,
our success and his success was not preordained.
Jensen started out working at a Denny's
as a dishwasher and a busboy.
So be nice to them in the future.
And he said that the lessons he's learned there,
the value of hard work, humility,
and hospitality are what helped him keep the faith
and persevere through some of NVIDIA's early challenges.
In just a few minutes, we'll hear from NVIDIA founder
and CEO Jensen Huang on his unwavering vision of the future
and where we're headed next,
stay tuned and have a great CES.
<v ->This is how intelligence is made.</v>
A new kind of factory,
generator of tokens,
the building blocks of AI
tokens have opened a new frontier,
the first step into an extraordinary world
where endless possibilities are born.
Tokens transform words into knowledge
and breathe life into images.
They turn ideas into videos
and help us safely navigate any environment.
Tokens teach robots to move like the masters,
inspire new ways to celebrate our victories.
<v ->A martini please.</v>
<v ->Coming right up.</v>
<v ->Thank you Adam.</v>
<v Narrator>And give us peace of mind when we need it most.</v>
<v ->Hi Roka.</v>
<v ->Hi, Emma, it's good to see you again.</v>
<v ->Hi, Emma, we're gonna take your blood sample today, okay?</v>
<v ->Don't worry, I'm gonna be here the whole time.</v>
<v Narrator>They bring meaning to numbers</v>
to help us better understand the world around us.
Predict the dangers that surround us.
And find cures for the threats within us.
Tokens can bring our visions to life.
And restore what we've lost.
<v Text To Speech>Zachary, I got my voice back, buddy.</v>
<v Narrator>They help us move forward,</v>
one small step at a time.
And one giant leap together.
And here is where it all begins.
<v Announcer>Welcome to the stage,</v>
NVIDIA founder and CEO, Jensen Huang.
<v ->Welcome to CES.</v>
Are you excited to be in Las Vegas?
Do you like my jacket?
I thought I'd go the other way from Gary Shapiro.
I'm in Las Vegas after all.
If this doesn't work out, if all of you object,
well just get used to it, I think,
I really think you have to let this sink in.
In another hour or so,
you're gonna feel good about it.
Well, welcome to NVIDIA.
In fact, you're inside NVIDIA's digital twin
and we're gonna take you to NVIDIA.
Ladies and gentlemen, welcome to NVIDIA.
You're inside our digital twin.
Everything here is generated by AI,
it has been an extraordinary journey, extraordinary year,
and it started in 1993.
<v Game Announcer>Ready, go.</v>
<v ->With NV1 we wanted to build computers that can do things</v>
that normal computers couldn't
and NV1 made it possible
to have a game console in your PC.
Our programming architecture was called UDA,
missing the letter C until a little while later,
but UDA, Unified Device Architecture
and the first developer for UDA
and the first application that ever worked on UDA
was SEGA'S Virtual Fighter.
Six years later,
we invented in 1999 the programmable GPU
and it started 20 years, 20 plus years of incredible advance
in this incredible processor called the GPU.
It made modern computer graphics possible.
And now 30 years later, Sega's Virtual Fighter
is completely cinematic.
This is the new Virtual Fighter project that's coming.
I just can't wait, absolutely incredible.
Six years after that,
six years after 1999, we invented CUDA
so that we could explain
or express the programmability of our GPUs
to a rich set of algorithms that could benefit from it.
CUDA initially was difficult
to explain, and it took years in fact,
it took approximately six years.
Somehow six years later,
six years later or so,
2012,
Alex Krizhevsky, Ilya Sutskever
and Geoff Hinton discovered CUDA,
used it to process AlexNet,
and the rest of it is history.
AI has been advancing at an incredible pace since,
started with perception AI.
We now can understand images and words
and sounds, to generative AI.
We can generate images and texts and sounds,
and now agentic AI,
AIs that can perceive, reason, plan, and act.
And then the next phase, some
of which we'll talk about tonight, physical AI, 2012.
Now magically, 2018,
something happened that was pretty incredible.
Google's transformer was released as Burt
and the world of AI really took off.
Transformers as you know,
completely changed the landscape
for artificial intelligence.
In fact, it completely changed the landscape
for computing altogether.
We recognized properly
that AI was not just a new application
with a new business opportunity,
but AI, more importantly, machine learning enabled
by transformers was going to fundamentally change
how computing works.
And today, computing is revolutionized
in every single layer, from hand coding,
instructions that run on CPUs
to create software tools that humans use.
We now have machine learning that creates
and optimizes neural networks that processes on GPUs
and creates artificial intelligence.
Every single layer
of the technology stack has been completely changed,
an incredible transformation in just 12 years.
Well, we can now understand information
of just about any modality.
Surely you've seen text and images
and sounds and things like that.
But not only can we understand those,
we can understand amino acids, we can understand physics,
we understand them, we can translate them and generate them.
The applications are just completely endless.
In fact, almost any AI application
that you see out there,
what modality is the input that it learnt from?
What modality of information did it translate to?
And what modality of information is it generating?
If you ask these three fundamental questions,
just about every single application could be inferred.
And so when you see application
after applications that are AI driven, AI native,
at the core of it, this fundamental concept is there.
Machine learning has changed
how every application is going to be built,
how computing will be done, and the possibilities beyond.
Well, GPUs,
GeForce in a lot of ways,
all of this with AI is the house that GeForce built.
GeForce enabled AI to reach the masses.
And now AI is coming home to GeForce.
There are so many things that you can't do without AI.
Let me show you some of it now.
That was real time computer graphics.
No computer graphics researcher,
no computer scientist would've told you that it is possible
for us to ray trace every single pixel at this point.
Ray tracing is a simulation of light.
The amount of geometry that you saw was absolutely insane.
It would've been impossible without artificial intelligence.
There are two fundamental things that we did.
We used of course, programmable shading
and ray traced acceleration
to produce incredibly beautiful pixels.
But then we have artificial intelligence be conditioned,
be controlled by that pixel
to generate a whole bunch of other pixels.
Not only is it able to generate other pixels spatially,
because it's aware of what the color should be,
it has been trained on a supercomputer back in NVIDIA.
And so the neural network
that's running on the GPU can infer
and predict the pixels that we did not render.
Not only can we do that, it's called DLSS.
The latest generation of DLSS also generates beyond frames.
It can predict the future,
generating three additional frames
for every frame that we calculate, what you saw,
if we just said four frames of what you saw,
because we're gonna render one frame and generate three.
If I said four frames at full HD, 4K,
that's 33 million pixels or so.
Out of that 33 million pixels, we computed only two.
It is an absolute miracle
that we can computationally,
computationally using programmable shaders
and our ray trace engine, ray tracing engine
to compute 2 million pixels
and have AI predict all of the other 33.
And as a result, we're able
to render at incredibly high performance
because AI does a lot less computation.
It takes, of course an enormous amount of training
to produce that, but once you train it,
the generation is extremely efficient.
So this is one of the incredible capabilities
of artificial intelligence
and that's why there's
so many amazing things that are happening.
We used GeForce to enable artificial intelligence,
and now artificial intelligence
is revolutionizing GeForce.
Everyone, today we're announcing our next generation,
the RTX Blackwell family.
Let's take a look.
Here it is, our brand new GeForce RTX 50 series,
Blackwell architecture.
The GPU is just a beast.
4,000 TOPs, four petaflops of AI,
three times higher than the last generation Ada.
And we need all of it to generate those pixels
that I showed you, 380 ray tracing teraflops
so that we could, for the pixels that we have
to compute,
compute the most beautiful image you possibly can.
And of course, 125 shader teraflops.
There is actually a concurrent shader teraflops as well
as an integer unit of equal performance.
So two dual shaders.
One is for floating point, one is for integer,
G7 memory from Micron, 1.8 terabytes per second.
Twice the performance of our last generation.
And we now have the ability to intermix AI workloads
with computer graphics workloads.
And one of the amazing things about this generation is
the programmable shader is also able
to now process neural networks.
So the shader is able to carry these neural networks
and as a result, we invented neural texture compression
and neural material shading.
As a result of that, you get these
amazingly beautiful images that are only possible
because we use AIs to learn the texture,
learn the compression algorithm,
and as a result, get extraordinary results.
Okay, so this is,
this is the brand new RTX Blackwell 5090.
Now even the mechanical design is a miracle.
Look at this, it's got two fans.
This whole graphics card is just one giant fan, you know,
so the question is, where's the graphics card,
is literally this big,
the voltage regulator design is state of the art,
incredible design.
The engineering team did a great job, so here it is.
Thank you.
Okay, so those are the speeds and feats.
So how does it compare?
Well, this is RTX 4090.
I know, I know many of you have one.
I know it, look, it's $1,599.
It is one of the best investments
you could possibly make.
For 1,599, you bring it home
to your $10,000 PC entertainment command center.
Isn't that right?
Don't tell me that's not true.
Don't be ashamed,
it's liquid cooled, fancy lights all over it.
You lock it when you leave.
It's the modern home theater, it makes perfect sense.
And now for $1,500 and 99, 1,599, you get to upgrade that
and turbocharge the living daylights out of it.
Well now with the Blackwell family,
RTX 5070, 4090 performance at 549.
Impossible without artificial intelligence,
impossible without the four TOPs, four teraops
of AI tensor cores, impossible without the G7 memories.
Okay, so 5070, 4090 performance, $549.
And here's the whole family,
starting from 5070 all the way up to 5090,
starting, of course,
we're producing at very large scale
availability starting January.
Well, it is incredible.
But we managed to put
these gigantic performance GPUs into a laptop.
This is a 5070 laptop for 1299,
this 5070 laptop has a 4090 performance.
I think there's one here somewhere, lemme show you this.
This is a, look at this thing here, let me, here.
There's only so many pockets,
ladies and gentlemen, Janine Paul.
So can you imagine,
you got this incredible graphics card here, Blackwell,
I'm gonna shrink it and put it in there.
Does that make any sense?
Well, you can't do that without artificial intelligence.
And the reason for that is
because we're generating most of the pixels,
using pixels, using our tensor cores.
So we ray trace only the pixels we need
and we generate using artificial intelligence
all of the other pixels we have.
As a result, the energy efficiency is
just off the charts.
The future of computer graphics is neural rendering,
the fusion of artificial intelligence and computer graphics.
And what's really amazing is, oh, here we go, thank you.
This is a surprisingly kinetic keynote
and what's really amazing is the family
of GPUs we're gonna put in here.
And so the 5090, the 5090
will fit into a laptop, a thin laptop.
That last laptop was 14, 14.9 millimeters.
You got a 5080, 5070 Ti and 5070.
Okay, so ladies and gentlemen, the RTX Blackwell family.
While GeForce brought AI
to the world, democratized AI.
Now AI has come back and revolutionized GeForce.
Let's talk about artificial intelligence.
Let's go to somewhere else at NVIDIA.
This is literally our office.
This is literally NVIDIA's headquarters.
Okay, so let's talk about AI.
The industry is chasing
and racing to scale artificial intelligence,
artificial intelligence,
and the scaling law is a powerful model.
It's an empirical law that has been observed
and demonstrated by researchers
and industry over several generations.
And the scaling laws says
that the more data you have, the training data
that you have, the larger model that you have,
and the more compute that you apply to it,
therefore, the more effective
or the more capable your model will become.
And so the scaling law continues.
What's really amazing is that now we're moving towards,
of course, and the internet is producing
about twice the amount of data every single year
as it did last year.
I think in the next couple years, we'll produce,
humanity will produce more data than all
of humanity has ever produced since the beginning.
And so we're still producing a gigantic amount of data,
and it's becoming multimodal, video and images and sound.
All of that data could be used
to train the fundamental knowledge,
the foundational knowledge of an AI.
But there are in fact two other scaling laws
that has now emerged, and it's somewhat intuitive.
The second scaling law is post-training scaling law,
post-training scaling law uses technology techniques
like reinforcement learning, human feedback.
Basically the AI produces
and generates answers based on a human query.
The human then of course gives a feedback.
It's much more complicated than that.
But that reinforcement learning system with a fair number
of very high quality prompts causes the AI
to refine its skills.
It could fine tune its skills for particular domains.
It could be better at solving math problems,
better at reasoning, so on and so forth.
And so it's essentially like having a mentor
or having a coach give you feedback
after you're done going to school.
And so you get tests,
you get feedback, you improve yourself.
We also have reinforcement learning AI feedback,
and we have synthetic data generation.
These techniques are rather akin to,
if you will, self-practice.
You know the answer to a particular problem
and you continue to try it until you get it right.
And so an AI could be presented with a very complicated
and a difficult problem
that is verifiable functionally.
And it has an answer that we understand,
maybe proving a theorem,
maybe solving a geometry problem.
And so these problems would cause the AI to produce answers.
And using reinforcement learning, you would learn
how to improve itself.
That's called post training.
Post training requires an enormous amount of computation,
but the end result produces incredible models.
We now have a third scaling law.
And this third scaling law has to do with
what's called test time scaling, test time scaling is
basically when you're being used, when you're using the AI,
the AI has the ability
to now apply a different resource allocation instead
of improving its parameters.
Now it's focused on deciding how much computation to use
to produce the answers it wants to produce.
Reasoning is a way of thinking about this.
Long thinking is a way to think about this.
Instead of a direct inference
or one shot answer, you might reason about it,
you might break down the problem into multiple steps.
You might generate multiple ideas and evaluate.
You know, your AI system would evaluate which one
of the ideas that you generated was the best one.
Maybe it solves the problem step by step,
so on and so forth.
And so now, test time scaling has proven
to be incredibly effective.
You are watching this sequence of technology
and this, all of these scaling laws emerge
as we see incredible achievements from ChatGPT to o1
to o3, and now Gemini Pro.
All of these systems are going through this journey,
step by step, by step of pre-training to post-training,
to test time scaling.
Well, the amount of computation that we need,
of course is incredible.
And we would like, in fact, we would like in fact
that society has the ability to scale the amount
of computation to produce more and more novel
and better intelligence.
Intelligence of course,
is the most valuable asset that we have.
And it can be applied to solve a lot
of very challenging problems.
And so scaling law,
it's driving enormous demand for NVIDIA computing.
It's driving an enormous demand
for this incredible chip we call Blackwell.
Let's take a look at Blackwell.
Well, Blackwell is in full production.
It is incredible what it looks like.
So first of all, there are some,
every single cloud service provider
now have systems up and running.
We have systems here from about 15,
excuse me, 15 computer makers.
It's being made about 200 different SKUs,
They're liquid cooled, air cooled, X86,
NVIDIA Grace CPU versions, NVLink 36, by two,
NVLinks 72 by one.
Whole bunch of different types of systems
so that we can accommodate
just about every single data center in the world.
Well, these systems are being currently manufactured
in some 45 factories.
It tells you how pervasive artificial intelligence is
and how much the industry is jumping
onto artificial intelligence in this new computing model.
Well, the reason why we're driving it so hard is
because we need a lot more computation.
And it's very clear.
It's very clear that,
Janine,
you know,
it's hard to tell,
you don't ever wanna reach your hands into a dark place.
Hang a second, is this a good idea?
All right.
Wait for it.
Wait for it.
I thought I was worthy.
Apparently Mjollnir didn't think I was worthy.
All right, this is my show and tell, this is show and tell.
So this NVLink system, this right here,
this NVLink system, this is GB 200, NVLink 72.
It is one and a half tons, 600,000 parts
approximately equal to 20 cars.
Twelve, a hundred and twenty kilowatts.
It has a spine behind it that connects all
of these GPUs together.
Two miles of copper cable,
5,000 cables.
This is being manufactured in 45 factories around the world.
We build them, we liquid cool 'em, we test them,
we disassemble 'em, ship the parts to the data centers
because it's one and a half tons.
We reassemble it outside the data centers and install 'em.
The manufacturing is insane.
But the goal of all of this is
because the scaling laws are driving computing so hard,
that this level of computation,
Blackwell over our last generation improves
the performance per watt by a factor of four,
performance per watt by a factor of four,
performance per dollar by a factor of three.
That's basically says that in one generation,
we reduce the cost of training these models
by a factor of three.
Or if you want to increase the size of your model
by a factor of three it's about the same cost.
But the important thing is this.
These are generating tokens that are being used by all
of us when we use ChatGPT
or when we use Gemini, use our phones in the future.
Just about all of these applications are gonna be consuming
these AI tokens.
And these AI tokens are being generated by these systems.
And every single data center is limited by power.
And so if the perf per watt of Blackwell is four times
our last generation, then the revenue
that could be generated, the amount of business
that can be generated in the data center is increased
by a factor of four.
And so these AI factory systems really are factories today.
Now, the goal of all of this is to,
so that we can create one giant chip, the amount
of computation we need is really quite incredible.
And this is basically one giant chip.
If we would've had to build a chip, one, here we go.
Sorry guys, you see that, that's cool.
Look at that, disco lights in here.
If we had to build this as one chip,
obviously this would be the size of the wafer,
but this doesn't include the impact of yield.
It would have to be probably three or four times the size.
But what we basically have here
is 72 Blackwell GPUs or 144 dyes.
This one chip here is 1.4 exaflops
the world's largest supercomputer, fastest supercomputer.
Only recently,
this entire room supercomputer,
only recently achieved an exaflop plus.
This is 1.4 exaflops of AI floating point performance.
It has 14 terabytes of memory.
But here's the amazing thing,
the memory bandwidth is 1.2 petabytes per second.
That's basically, basically the entire internet traffic
that's happening right now.
The entire world's internet traffic is being processed
across these chips.
Okay, and we have 130 trillion transistors in total,
2,592 CPU cores, whole bunch of networking.
And so these, I wish I could do this, I don't think I will.
So these are the Blackwells.
These are our Connect X networking chips.
These are the NVLink.
And we're trying to pretend
about the NVLink spine.
But that's not possible, okay?
And these are all of the HBM memories,
This is what we're trying to do, and this is the miracle.
This is the miracle of the Blackwell system.
The Blackwell die's right here.
It is the largest single chip the world's ever made,
but yet the miracle is really in addition to that,
this is the Grace Blackwell system.
Well, the goal of all of this, of course,
is so that we can, thank you, thanks.
Boy, is there a chair?
I could sit down for a second.
Can I have a Michelob Ultra?
How is it possible?
We're in the Michelob Ultra Stadium,
it's like coming to NVIDIA and we don't have a GPU for you.
So, we need an enormous amount of computation
because we wanna train larger and larger models.
And these inferences,
these inferences used to be one inference,
but in the future, the AI's gonna be talking to itself.
It's gonna be thinking,
it's gonna be internally reflecting, processing.
So today, when the tokens are being generated at you,
so long as it's coming out at 20
or 30 tokens per second, it's basically as fast
as anybody can read, however, in the future.
And right now with GPT o1, you know, with the new,
the Gemini Pro and the new,
the o1, o3 models, they're talking to themselves,
reflecting, they're thinking.
And so as you can imagine,
the rate at which the tokens could be ingested is
incredibly high.
And so we need the token rates,
the token generation rates to go way up.
And we also have to drive the cost way down simultaneously
so that the quality of service can be extraordinary.
The cost to customers can continue to be low
and AI will continue to scale.
And so that's the fundamental purpose.
The reason why we created NVLink, well,
one of the most important things
that's happening in the world of enterprise is agentic AI.
Agentic AI basically is a perfect example
of test time scaling.
It's a AI, is a system of models.
Some of it is understanding, interacting with the customer,
interacting with the user.
Some of it's maybe retrieving information,
retrieving information from storage,
a semantic AI system like a RAG, maybe it's going on
to the internet, maybe it's studying a PDF file.
And so it might be using tools,
it might be using a calculator,
and it might be using a generative AI
to generate charts and such.
And it iterates, it's taking the problem you gave it,
breaking it down step by step.
And it's iterating through all these different models.
Well, in order to respond to a customer in the future,
in order for AI to respond, it used to be ask a question,
answer starts spewing out.
In the future, you ask a question, a whole bunch
of models are gonna be working in the background.
And so test time scaling, the amount of computation used
for inferencing is gonna go through the roof.
It's gonna go through the roof
because we want better and better answers.
Well, to help the industry build agentic AI,
our go-to market is not direct to enterprise customers.
Our go-to market is we work
with software developers in the IT ecosystem
to integrate our technology
to make possible new capabilities, just like we did
with CUDA libraries.
We now wanna do that with AI libraries.
And just as the computing model of the past has APIs
that are doing computer graphics or doing linear algebra
or doing fluid dynamics, in the future, on top
of those acceleration libraries,
CUDA acceleration libraries, we'll have AI libraries.
We've created three things
for helping the ecosystem build agentic AI, NVIDIA NIMs,
which are essentially AI microservices all packaged up.
It takes all of this really complicated CUDA software,
CUDA DNN, Cutlass or Tensor RTLM or Triton
or all of these different really complicated software.
And the model itself, we package it up, we optimize it,
we put it into a container,
and you could take it wherever you like.
And so we have models for vision,
for understanding languages, for speech,
for animation, for digital biology.
And we have
some new exciting models coming for physical AI.
And these AI models run in every single cloud
because NVIDIA's GPUs are now available
in every single cloud.
It's available in every single OEM.
So you could literally take these models,
integrate it into your software packages,
create AI agents that run on Cadence,
or they might be service now agents,
or they might be SAP agents
and they could deploy it to their customers
and run it wherever the customers wanna run the software.
The next layer is what we call NVIDIA NeMo.
NeMo is essentially
a digital employee onboarding
and training evaluation system.
In the future, these AI agents are
essentially digital workforce
that are working alongside your employees, working along,
doing things for you on your behalf.
And so the way that you would bring these specialized agents
into your, these special agents, into your company is
to onboard them, just like you onboard an employee.
And so we have different libraries that helps
these AI agents be trained for the type of, you know,
language in your company,
maybe the vocabulary unique to your company.
The business process is different,
the way you work is different.
So you would give them examples of
what the work products should look like,
and they would try to generate it,
and you would give a feedback
and then you would evaluate them, so on and so forth.
And so that,
and you would guardrail 'em, you say,
these are the things that you're not allowed to do.
These are the things you're not allowed to say.
And we even give them access to certain information.
Okay, so that entire pipeline,
digital employee pipeline is called NeMo.
In a lot of ways,
the IT department of every company is going
to be the HR department of AI agents in the future.
Today they manage
and maintain a bunch of software
from the IT industry.
In the future, they'll maintain, you know, nurture,
onboard and improve a whole bunch of digital agents
and provision 'em to the companies to use.
Okay, and so your IT department's gonna become
kind of like AI agent HR.
And on top of that, we provide a whole bunch of blueprints
that our ecosystem could take advantage of.
All of this is completely open source.
And so you could take it and modify the blueprints.
We have blueprints for all kinds
of different types of agents.
Well, today we're also announcing that we're doing something
that's really cool and I think really clever.
We're announcing a whole family of models
that are based off of Llama,
the NVIDIA Llama Nemotron Language Foundation Models.
Llama 3.1 is a complete phenomenon.
The download of Llama 3.1 from Meta,
650,000 times, something like that.
It has been derived and turned into other models,
about 60,000 other different models.
It is singularly the reason why
just about every single enterprise
and every single industry has been activated
to start working on AI.
Well, the thing that we did was we realized
that the Llama models really could be better fine tuned
for enterprise use.
And so we fine tune 'em using our expertise
and our capabilities
and we turn 'em into the Llama Nemotron Suite
of open models.
There are small ones that interact
and very, very fast response time, extremely small.
They're super, what we call super, Llama Nemotron Supers.
They're basically your mainstream versions of your models
or your ultra model.
The ultra model could be used to be a teacher model
for a whole bunch of other models.
It could be a reward model, evaluator,
a judge for other models to create answers
and decide whether it's a good answer or not.
Give, basically give feedback to other models.
It could be distilled in a lot of different ways.
Basically a teacher model, a knowledge distillation model.
Very large, very capable.
And so all of this is now available online.
Well, these models are incredible.
It's number one in leaderboards for chat,
leaderboard for instruction, leaderboard for retrieval.
So the different types of functionalities necessary
that are used in AI agents around the world.
These are gonna be incredible models for you.
We're also working with the ecosystem.
These tech, all of our NVIDIA AI technologies are integrated
into the IT industry.
We have great partners
and really great work being done at ServiceNow, at SAP
at Siemens for industrial AI.
Cadence is doing great work, Synopsis is doing great work.
I'm really proud of the work that we do with Perplexity.
As you know, they revolutionize search.
Yeah, really fantastic stuff.
Codeium, every software engineer in the world,
this is going to be the next giant AI application.
Next giant AI service period is software coding.
Everybody is going
to have a software assistant helping them code.
If not, obviously you're just,
you're gonna be way less productive
and create lesser good code.
And so this is 30 million.
There's a billion knowledge workers in the world.
It is very, very clear.
AI agents is probably the next robotics industry
and likely to be a multi-trillion dollar opportunity.
Well, let me show you some of the blueprints that we created
and some of the work that we've done with our partners
with these AI agents.
<v Announcer 2>AI agents are the new digital workforce,</v>
working for and with us.
AI agents are a system of models
that reason about a mission, break it down into tasks
and retrieve data or use tools
to generate a quality response.
NVIDIA's agentic AI building blocks,
NIM pre-trained models and NeMo framework
let organizations easily develop AI agents
and deploy them anywhere.
We will onboard and train our agentic workforces
on our company's methods like we do for employees.
AI agents are domain specific task experts.
Let me show you four examples.
For the billions of knowledge workers and students,
AI research assistant agents ingest complex documents
like lectures, journals, financial results,
and generate interactive podcasts for easy learning.
<v ->By combining a U-Net regression model</v>
with a diffusion model,
core diff can downscale global weather forecast down
from 25 kilometers to two kilometers.
<v Announcer 2>Developers, like at NVIDIA,</v>
manage software security AI agents
that continuously scan software for vulnerabilities,
alerting developers to what action is needed.
Virtual lab AI agents help researchers design
and screen billions of compounds
to find promising drug candidates faster than ever.
And video analytics AI agents built
on an NVIDIA metropolis blueprint, including NVIDIA Cosmos,
Nemotron vision language models, Llama Nemotron LLMs
and NeMo Retriever, Metropolis agents analyze content
from the billions of cameras,
generating 100,000 petabytes of video per day.
They enable interactive search, summarization
and automated reporting,
and help monitor traffic flows, flagging congestion
or danger.
In industrial facilities, they monitor processes
and generate recommendations or improvement.
Metropolis agents centralize data from hundreds of cameras
and can reroute workers or robots when incidents occur.
The age of agentic AI is here for every organization.
<v ->Okay.</v>
That was the first pitch
at a baseball game, that was not generated.
I just felt that none of you were impressed, okay,
so AI was created in the cloud
and for the cloud, AI is created in the cloud, for the cloud
and for enjoying AI on phones,
of course, it's perfect,
very, very soon we're have a continuous AI
that's gonna be with you.
And when you use those Meta glasses, you could of course
point at something, look at something and ask it.
You know, whatever information you want.
And so AI is perfect in the cloud,
was created in the cloud, it's perfect in the cloud.
However, we would love to be able to take
that AI everywhere.
I've mentioned already that you could take NVIDIA AI
to any cloud, but you could also put it inside your company.
But the thing that we wanna do more than anything is put it
on our PC as well.
And so, as you know,
Windows 95 revolutionized the computer industry.
It made possible this new suite of multimedia services
and then changed the way that applications
was created forever.
Windows 95.
This model of computing
of course is not perfect for AI.
And so the thing that we would like to do is we would like
to have in the future your AI basically become
your AI assistant.
And instead of just the 3D APIs
and the sound APIs and the video APIs,
you would have generative APIs, generative APIs for 3D
and generative APIs for language
and generative AI for sound and so on and so forth.
And we need a system that makes
that possible while leveraging
the massive investment that's in the cloud.
There's no way that we could, the world can create
yet another way of programming AI models.
It's just not gonna happen.
And so if we could figure out a way
to make Windows PC a world-class AI pc,
it would be completely awesome.
And it turns out the answer is Windows.
It's Windows WSL2, Windows WSL2,
Windows WSL2 basically it's two operating systems
within one.
It works perfectly, it's developed for developers
and it's developed so
that you can have access to bare metal.
WSL2 has been optimized for cloud native applications.
It is optimized for,
and very importantly it's been optimized for CUDA.
And so WSL2 supports CUDA perfectly
out of the box as a result.
Everything that I showed you with NVIDIA,
NIMs, NVIDIA NeMo, the blueprints
that we develop that are gonna be up in ai.nvidia.com,
so long as the computer fits it,
so long as you can fit that model.
And we're gonna have many models that fit.
Whether it's vision models or language models
or speech models or these animation,
human digital human models, all kinds
of different types of models are gonna be perfect
for your PC and it would, you download it
and it should just run.
And so our focus is to turn Windows WSL2, Windows PC
into a target first class platform that we will support
and maintain for as long as we shall live.
And so this is an incredible thing
for engineers and developers everywhere.
Let me show you something that we can do with that.
This is one of the examples
of a blueprint we just made for you.
<v Announcer 3>Generative AI synthesizes amazing images</v>
from simple text prompts.
Yet image composition can be challenging
to control using only words.
With NVIDIA NIM microservices,
creators can use simple 3D objects
to guide AI image generation.
Let's see how a concept artist can use this technology
to develop the look of a scene.
They start by laying out 3D assets created by hand
or generated with AI.
Then use an image generation NIM, such as Flux
to create a visual that adheres to the 3D scene,
add or move objects to refine the composition,
change camera angles to frame the perfect shot,
or reimagine the whole scene with a new prompt.
Assisted by generative AI and NVIDIA NIM,
an artist can quickly realize their vision.
<v ->NVIDIA AI for your PCs,</v>
hundreds of millions of PCs in the world with Windows.
And so we could get 'em ready for AI.
OEMs, all the PC OEMs we work with, just basically all
of the world's leading PC OEMs are gonna get their PCs ready
for this stack.
And so AI PCs are coming to a home near you.
Linux is good.
Okay, let's talk about physical AI.
Speaking of Linux, let's talk about physical AI.
So physical AI, imagine, imagine,
whereas your large language model, you give it your context,
your prompt on the left,
and it generates tokens, one at a time
to produce the output.
That's basically how it works.
The amazing thing is this model in the middle is
quite large, has billions of parameters.
The context length is incredibly large
because you might decide to load in a PDF, in my case,
I might load in several PDFs before I ask it a question.
Those PDFs are turned into tokens.
The attention, the basic attention characteristic
of a transformer has every single token find
its relationship and relevance against every other token.
So you could have hundreds of thousands of tokens
and the computational load increases quadratically.
And it does this, all of the parameters,
all of the input sequence,
process it through every single layer
of the transformer and it produces one token.
That's the reason why we needed Blackwell.
And then the next token is produced,
when the current token is done,
it puts the current token into the input sequence
and takes that whole thing and generates the next token.
It does it one at a time, this is the transformer model.
It's the reason why it's so, so incredibly effective.
Computationally demanding.
What if instead of PDFs, it's your surrounding.
And what if instead of the prompt a question,
it's a request, go over there
and pick up that, you know, that box and bring it back.
And instead of what is produced in tokens, it's text,
it produces action tokens.
Well, that I just described is a very sensible thing
for the future of robotics.
And the technology is right around the corner.
But what we need to do is we need to create the effective,
effectively the world model of, you know,
as opposed to GPT, which is a language model.
And this world model has to understand the language
of the world, has to understand physical dynamics,
things like gravity and friction and inertia.
It has to understand geometric and spatial relationships.
It has to understand cause and effect.
If you drop something it falls to ground,
if you, you know, poke at it, it tips over,
it has to understand object permanence.
If you roll a ball over the kitchen counter,
when it goes off the other side,
the ball didn't leave into another quantum universe,
that's still there.
And so all of these types of understanding is
intuitive understanding that we know
that most models today have a very hard time with.
And so we would like to create a world,
we need a world foundation model.
Today we're announcing a very big thing.
We're announcing NVIDIA Cosmos,
a world foundation model that is designed,
that was created to understand the physical world.
And the only way for you
to really understand this is to see it.
Let's play it.
<v Announcer 4>The next frontier of AI is physical AI,</v>
model performance is directly related to data availability,
but physical world data is costly to capture,
curate, and label.
NVIDIA Cosmos is a world foundation,
model development platform to advance physical AI.
It includes auto regressive world foundation models,
diffusion based world foundation models, advanced tokenizers
and an NVIDIA CUDA and AI accelerated data pipeline.
Cosmos models ingest text, image or video prompts
and generate virtual world states as videos.
Cosmos generations prioritize the unique requirements
of AV and robotics use cases like real world environments,
lighting and object permanence.
Developers use NVIDIA Omniverse
to build physics based geospatially accurate scenarios.
Then output Omniverse renders into Cosmos,
which generates photoreal physically based synthetic data.
Whether diverse objects
or environments,
conditions like weather,
or time of day,
or edge case scenarios.
Developers use Cosmos to generate worlds
for reinforcement learning AI feedback
to improve policy models
or to test and validate model performance.
Even across multisensor views,
Cosmos can generate tokens in real time, bringing the power
of foresight and multiverse simulation to AI models,
generating every possible future
to help the model select the right path.
Working with the world's developer ecosystem,
NVIDIA is helping advance the next wave of physical AI.
<v ->NVIDIA Cosmos,</v>
NVIDIA Cosmos,
NVIDIA Cosmos, the world's first world foundation model.
It is trained on 20 million hours of video.
The 20 million hours of video focuses
on physical dynamic things.
So dynamic nature, nature themes, humans walking,
hands moving, manipulating things, you know,
things that are fast camera movements.
It's really about teaching the AI
not about generating creative content,
but teaching the AI to understand the physical world.
And from this, with this physical AI
there are many downstream things
that we could do as a result.
We could do synthetic data generation to train models.
We could distill it
and turn it into effectively the seed,
the beginnings of a robotics model.
You could have it generate multiple physically based,
physically plausible scenarios of the future.
Basically do a Doctor Strange.
You could,
because this model understands the physical world,
of course you saw a whole bunch of images generated,
this model understanding the fiscal world.
It also could do of course, captioning.
And so it could take videos, caption it incredibly well,
and that captioning
and the video could be used to train large language models,
multimodality large language models.
And so you could use this technology
to use this foundation model to train robotics, robots,
as well as large language models.
And so this is the NVIDIA Cosmos.
The platform has an auto regressive model
for real time applications.
Has diffusion model
for a very high quality image generation.
It's incredible tokenizer,
basically learning the vocabulary of real world
and a data pipeline.
So that if you would like to take all of this
and then train it on your own data, this data pipeline,
because there's so much data involved,
we've accelerated everything end to end for you.
And so this is the world's first data processing pipeline
that's CUDA accelerated as well as AI accelerated.
All of this is part of the Cosmos platform.
And today we're announcing that Cosmos is open licensed,
it's open and available on GitHub.
We hope that this moment
and there's a small, medium, large
for very fast models, you know, mainstream models
and also teacher models.
Basically not knowledge transfer models.
Cosmos World Foundation model being open,
we really hope will do for the world of robotics
and industrial AI
what Llama 3 has done for enterprise AI,
the magic happens when you connect Cosmos to Omniverse.
And the reason fundamentally is this,
Omniverse is a physics grounded,
not physically grounded, but physics grounded.
It's algorithmic physics,
principled physics simulation, grounded system.
It's a simulator, when you connect that to Cosmos,
it provides the grounding, the ground truth that can control
and to condition the Osmos generation.
As a result, what comes out of Osmos is grounded on truth.
This is exactly the same idea
as connecting a large language model to a RAG,
to a Retrieval Augmented Generation system.
You want to ground the AI generation on ground truth.
And so the combination of the two gives you
a physically simulated,
a physically grounded multiverse generator.
And the application,
the use cases are really quite exciting.
And of course for robotics, for industrial applications,
it is very, very clear.
This Cosmos plus,
Omniverse plus Cosmos represents the third computer
that's necessary for building robotics systems.
Every robotics company will ultimately have
to build three computers.
A robotics, the robotics system could be factory,
the robotic system could be a car, it could be a robot.
You need three fundamental computers.
One computer of course to train the AI
we call it the DGX computer to train the AI.
Another of course when you're done to deploy the AI,
we call that AGX, that's inside the car in the robot
or in an AMR or you know,
in a stadium or whatever it is.
These computers are at the edge and they're autonomous.
But to connect the two, you need a digital twin.
And this is all the simulations that you were seeing.
The digital twin is where the AI that has been trained goes
to practice to be refined,
to do its synthetic data generation, reinforcement learning,
AI feedback such and such.
And so it's the digital twin of the AI.
These three computers are gonna be working interactively.
NVIDIA's strategy for the industrial world.
And we've been talking about this for some time,
is this three computer system, you know,
instead of a three three body problem,
we have a three computer solution.
And so it's the NVIDIA robotics.
So let me give you three examples.
All right, so the first example is how we apply,
apply all of this to industrial digitalization.
There are millions of factories, hundreds of thousands
of warehouses that's basically, it's the backbone
of a $50 trillion manufacturing industry.
All of that has to become software defined.
All of it has to have automation in the future
and all of it will be infused with robotics.
Well we're partnering with KION,
the world's leading warehouse automation solutions provider
and Accenture,
the world's largest professional services provider
and they have a big focus in digital manufacturing.
And we're working together to create something
that's really special and I'll show you that in a second.
But our go-to market is essentially the same as all
of the other software platforms
and all the technology platforms that we have,
through the developers and ecosystem partners.
And we have just a growing number
of ecosystem partners connecting to Omniverse.
And the reason for that is very clear.
Everybody wants to digitalize the future of industries.
There's so much waste, so much opportunity for automation
in that $50 trillion of the world's GDP.
So let's take a look at that.
This one example that we're doing with KION and Accenture.
<v Announcer 5>KION, the supply chain solution company.</v>
Accenture, a global leader in professional services
and NVIDIA, are bringing physical AI
to the $1 trillion warehouse and distribution center market.
Managing high performance warehouse logistics
involves navigating a complex web of decisions influenced
by constantly shifting variables.
These include daily and seasonal demand changes,
space constraints, workforce availability,
and the integration of diverse robotic
and automated systems.
And predicting operational KPIs
of a physical warehouse is nearly impossible today.
To tackle these challenges,
KION is adopting Mega, an NVIDIA Omniverse blueprint
for building industrial digital twins to test
and optimize robotic fleets.
First, KION's warehouse management solution assigns tasks
to the industrial AI brains in the digital twin,
such as moving a load from a buffer location
to a shuttle storage solution.
The robot's brains are in a simulation
of a physical warehouse, digitalized into Omniverse,
using OpenUSD connectors to aggregate CAD, video
and image to 3D, LiDAR to Point Cloud,
and AI generated data.
The fleet of robots execute tasks by perceiving
and reasoning
about their Omniverse digital twin environment,
planning their next motion and acting,
the robot brains can see the resulting state
through sensor simulations and decide their next action.
The loop continues while Mega precisely tracks the state
of everything in the digital twin.
And now KION can simulate infinite scenarios at scale
while measuring operational KPIs such as throughput,
efficiency and utilization,
all before deploying changes to the physical warehouse,
together with NVIDIA,
KION and Accenture are reinventing industrial autonomy.
<v ->That's incredible, everything is in simulation.</v>
In the future,
in the future, every factory will have a digital twin.
And that digital twin operates exactly
like the real factory.
And in fact you could use Omniverse with Cosmos
to generate a whole bunch of future scenarios.
And you pick, then an AI decides which one
of the scenarios are the most optimal for whatever KPIs.
And that becomes the programming constraints,
the program if you will, the AIs
that will be deployed into the real factories.
The next example, autonomous vehicles.
The AV revolution has arrived,
after so many years with Waymo success
and Tesla's success.
It is very, very clear.
Autonomous vehicles has finally arrived,
well our offering to this industry is the three computers,
the training systems to train the AIs,
the simulation systems
and the synthetic data generation systems, Omniverse
and now Cosmos and also the computer that's inside the car.
Each car company might work
with us in a different way, use one or two
or three of the computers.
We're working with
just about every major car company around the world.
Waymo and Zoox and Tesla of course in their data center.
BYD, the largest EV company in the world.
JLR's got a really cool car coming,
Mercedes has a fleet of cars coming with NVIDIA,
starting this year going to production.
And I'm super, super pleased to announce that today,
Toyota and NVIDIA are gonna partner together
to create their next generation AVs.
Just so many, so many cool companies.
Lucid and Rivian and Xiaomi,
and of course Volvo, just so many different companies.
Waabi is building self-driving trucks.
Aurora, we announced this week also
that Aurora is gonna use NVIDIA
to build self-driving trucks.
Autonomous, a hundred million cars built each year.
A billion cars, vehicles on a road all over the world,
a trillion miles that are driven around the world each year.
That's all going to be either highly autonomous
or you know, fully autonomous coming up.
And so this is gonna be a very large, very large industry.
I predict that this will likely be
the first multi-trillion dollar robotics industry.
This business for us.
Notice in just a few of these cars that are starting
to ramp into the world, our business is already $4 billion
and this year probably on a run rate about $5 billion.
So really significant business already.
This is gonna be very large.
Well today we're announcing
that our next generation processor for the car,
our next generation computer for the car is called Thor.
I have one right here, hang on a second.
Okay, this is Thor, this is Thor.
This is a robotics computer.
This is a robotics computer.
Takes sensors at just a madness amount
of sensor information, process it, you know,
umpteen cameras, high resolution, radars, lidars,
they're all coming into this chip.
And this chip has to process all that sensor,
turn 'em into tokens, put 'em into a transformer
and predict the next path.
And this AV computer is now in full production.
Thor is 20 times the processing capability
of our last generation, Orin, which is really the standard
of autonomous vehicles today.
And so this is just really quite, quite incredible.
Thor is in full production.
This robotics processor by the way,
also goes into a full robot.
And so it could be an AMR, it could be a human or robot,
it could be the brain, it could be the manipulator.
This processor basically is
a universal robotics computer.
The second part of our drive system
that I'm incredibly proud of is the dedication to safety.
DriveOS I'm pleased to announce is now
the first software defined programmable AI computer
that has been certified up
to ASIL-D, which is the highest standard
of functional safety for automobiles.
The only and the highest.
And so I'm really, really proud of this, ASIL-D, ISO 26262.
It is the work of some 15,000 engineering years.
This is just extraordinary work.
And as a result of that,
CUDA is now a functional safe computer.
And so if you're building a robot, NVIDIA CUDA, yep.
Okay, so now I wanted to,
I told you I was gonna show you what would we use Omniverse
and Cosmos to do in the context of self-driving cars.
And you know, today, instead of showing you a whole bunch
of videos of cars driving on the road,
I'll show you some of that too.
But I wanna show you how we use the car
to reconstruct digital twins automatically using AI
and use that capability to train future AI models.
Okay, let's play it.
<v Announcer 6>The autonomous vehicle revolution is here.</v>
Building autonomous vehicles like all robots
requires three computers.
NVIDIA DGX, to train AI models, Omniverse to test drive
and generate synthetic data,
and Drive AGX, a super computer in the car.
Building safe autonomous vehicles
means addressing edge scenarios,
but real world data is limited.
So synthetic data is essential for training.
The autonomous vehicle data factory,
powered by NVIDIA Omniverse, AI models
and Cosmos generates synthetic driving scenarios
that enhance training data by orders of magnitude.
First, OmniMap fuses map
and geospatial data to construct drivable 3D environments.
Driving scenario variations can be generated
from replay drive logs or AI traffic generators.
Next, a neural reconstruction engine
uses autonomous vehicle sensor logs
to create high fidelity 4D simulation environments.
It replays previous drives in 3D
and generates scenario variations to amplify training data.
Finally, Edify 3DS automatically searches
through existing asset libraries
or generates new assets to create sim ready scenes.
The Omniverse scenarios are used to condition Cosmos
to generate massive amounts of photorealistic data,
reducing the sim to real gap
and with text prompts generate near infinite variations
of the driving scenario.
With Cosmos Nemotron video search,
the massively scaled synthetic dataset,
combined with recorded drives can be curated
to train models.
NVIDIA's AI data factory scales hundreds
of drives into billions of effective miles.
Setting the standard for safe
and advanced autonomous driving.
<v ->Isn't that incredible, we</v>
take thousands of drives
and turn 'em into billions of miles.
We are going to have mountains of training data
for autonomous vehicles.
Of course, we still need actual cars on the road.
Of course, we will continuously collect data
for as long as we shall live.
However, synthetic data generation using this multiverse,
physically based, physically grounded capability
so that we generate data for training AIs
that are physically grounded and accurate
and or plausible so that we could have enormous amount
of data to train with, the AV industry is here.
This is an incredibly exciting time.
Super, super, super excited about the next several years.
I think you're gonna see,
just as computer graphics was revolutionized
such incredible pace, you're gonna see the pace
of AV development increasing tremendously
over the next several years.
I think,
I think the next part is robotics.
So.
You remember robots,
my friends.
The ChatGPT moment
for general robotics is just around the corner.
And in fact, all of the enabling technologies
that I've been talking about is going to make it possible
for us in the next several years
to see very rapid breakthroughs,
surprising breakthroughs in general robotics.
Now, the reason why general robotics is
so important is whereas robots with tracks
and wheels require special environments to accommodate them,
there are three robots, three robots in the world
that we can make that require no green fields.
Brownfield adaptation is perfect.
If we could possibly build these amazing robots,
we could deploy them in exactly the world
that we've built for ourselves.
These three robots are one,
agentic robots and agentic AI,
because you know, they're information workers.
So long as they could accommodate the computers
that we have in our offices, it's gonna be great.
Number two, self-driving cars.
And the reason for that is we spent a hundred plus years
building roads and cities,
and then number three, humanoid robots.
If we have the technology to solve these three,
this will be the largest technology industry
the world's ever seen.
And so we think that robotics era is just around the corner.
The critical capability is how to train these robots.
In the case of humanoid robots,
the imitation information is rather hard to collect.
And the reason for that is
in the case of car, you just drive it.
We're driving cars all the time.
In the case of these humanoid robots,
the imitation information
that the human demonstration is rather laborious to do.
And so we need to come up with a clever way to take hundreds
of demonstrations, thousands of human demonstrations,
and somehow use artificial intelligence
and Omniverse to synthetically generate millions
of synthetically generated motions.
And from those motions, the AI can learn
how to perform a task.
Let me show you how that's done.
<v Announcer 7>Developers around the world</v>
are building the next wave
of physical AI embodied robots, humanoids.
Developing general purpose robot models requires
massive amounts of real world data, which is costly
to capture and curate.
NVIDIA Isaac GR00T helps tackle these challenges,
providing humanoid robot developers
with four things, robot foundation models,
data pipelines, simulation frameworks,
and a Thor robotics computer.
The NVIDIA Isaac GR00T Blueprint
for Synthetic Motion Generation is a simulation workflow
for imitation learning, enabling developers
to generate exponentially large data sets
from a small number of human demonstrations.
First, GR00T-Teleop enables skilled human workers
to portal into a digital twin
of their robot using the Apple Vision Pro.
This means operators can capture data
even without a physical robot,
and they can operate the robot
in a risk-free environment, eliminating the chance
of physical damage or wear and tear.
To teach a robot a single task,
operators capture motion trajectories through a handful
of teleoperated demonstrations, then use GR00T-Mimic
to multiply these trajectories into a much larger data set.
Next, they use GR00T-Gen, built on Omniverse
and Cosmos for domain randomization
and 3D to real upscaling,
generating an exponentially larger data set.
The Omniverse and Cosmos multiverse simulation engine
provides a massively scaled data set
to train the robot policy.
Once the policy is trained,
developers can perform software in the loop testing
and validation in Isaac Sim
before deploying to the real robot.
The age of general robotics is arriving,
powered by NVIDIA Isaac GR00T.
<v ->We're gonna have mountains of data to train robots with.</v>
NVIDIA Isaac GR00T, NVIDIA Isaac GR00T.
This is our platform to provide technology,
platform technology elements to the robotics industry
to accelerate the development of general robotics.
And well, I have one more thing that I wanna show you.
None of this would be possible
if not for this incredible project
that we started about a decade ago.
Inside the company was called Project DIGITS,
Deep Learning GPU Intelligence Training System, DIGITS.
Well before we launched it,
I shrunk it to DGX and to harmonize it
with RTX, AGX, OVX
and all of the other Xs that we have in the company.
And it really revolutionized,
DGX-1 really revolutionized, where's DGX-1?
The DGX-1 revolutionized artificial intelligence.
The reason why we built it was
because we wanted to make it possible for researchers
and startups to have an out of the box AI supercomputer.
Imagine the way supercomputers were built in the past.
You really have to build your own facility
and you have to go build your own infrastructure
and really engineer it into existence.
And so we created a supercomputer for AI,
for AI development, for researchers and startups.
That comes literally out of the box.
I delivered the first one to a startup company
in 2016 called OpenAI,
and Elon was there, and Ilia Sutskever was there,
and many of NVIDIA engineers were there.
And we celebrated the arrival of DGX-1.
And obviously it revolutionized artificial intelligence
and computing.
But now artificial intelligence is everywhere.
It's not just in researchers and startup labs.
You know, we want artificial intelligence.
As I mentioned in the beginning of our talk,
this is now the new way of doing computing.
This is the new way of doing software.
Every software engineer, every engineer,
every creative artist, everybody who uses computers today
as a tool will need a AI supercomputer.
And so I just wished, I just wish that DGX-1 was smaller
and, you know, so you know, imagine
ladies and gentlemen.
This is NVIDIA's latest AI supercomputer.
And it's fondly called Project DIGITS right now.
And if you have a good name for it, reach out to us.
Here's the amazing thing, this is an AI supercomputer.
It runs the entire NVIDIA AI stack.
All of NVIDIA software runs on this.
DGX Cloud runs on this, this sits,
well somewhere and it's wireless
or, you know, connected to your computer.
It's even a workstation if you like it to be.
And you could access it, you could, you could reach it
like a cloud supercomputer.
And NVIDIA's AI works on it,
and it's based on a super secret chip
that we've been working on called GB110,
the smallest Grace Blackwell that we make.
And I have, well, you know what?
Let's show everybody inside.
Isn't this just, isn't just, it's just so cute.
And this is the chip that's inside,
it is in production.
This top secret chip we did in collaboration,
the CPU, the Grace CPU is built for NVIDIA
in collaboration with MediaTek.
They're the world's leading SOC company.
And they worked with us to build this CPU,
this CPU SOC and connect it with chip to chip NVLink
to the Blackwell GPU.
And this little thing here is in full production.
We're expecting this computer to be available
around May timeframe.
And so it's coming at you.
It's just incredible what we could do.
And it's just, I think it's, you really,
I was trying to figure out
do I need more hands or more pockets?
All right, so imagine this is what it looks like.
You know, who doesn't want one of those?
And if you use PC, Mac,
you know anything,
because you know, it is a cloud platform.
It's a cloud computing platform that sits on your desk.
You could also use it as a Linux workstation if you like.
If you would like to have double DIGITS,
this is what it looks like.
And you connect it together with Connect X
and it has Nickel, GPU Direct,
all of that out of the box.
It's like a supercomputer.
Our entire supercomputing stack is available.
And so NVIDIA Project DIGITS.
Okay, well let me tell you what I told you.
I told you that we are in production
with three new Blackwells.
Not only is the Grace Blackwell supercomputers,
NVLink 72s in production all over the world.
We now have three new Blackwell systems in production.
One amazing AI foundational, world foundation model.
The world's first physical AI foundation model is open,
available to activate the world's industries of robotics
and such, and three,
and three robotics, three robots working on agentic AI,
humanoid robots and self-driving cars.
It's been an incredible year.
I wanna thank all of you for your partnership.
Thank all of you for coming.
I made you a short video to reflect on last year
and look forward to the next year, play please.
Have a great CES, everybody, happy new year, thank you.