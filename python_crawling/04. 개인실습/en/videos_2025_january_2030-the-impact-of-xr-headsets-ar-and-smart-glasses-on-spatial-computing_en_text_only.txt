- Hello, good morning.
Thank you for joining us this morning
and for making your way,
way over into the West Hall.
It's quite a hike
and I'm really pleased that
you're here to join us.
I'm really pleased also
to have a really good panel this morning.
On this particular
topic, this is a subject
that I've been studying for over 20 years
and have worked on various
projects, both in VR, AR,
smart glasses, et cetera,
over the last seven or eight years.
And it's really captured my
imagination and my attention.
I've spent a lot of time working on it.
And this particular CES we did,
we did actually our first
panel on, I think we,
I think last year we did it
on the metaverse and glasses.
But it was very clear
that in the last year,
especially the term
spatial computing has come
to the forefront, not only
because Apple pushed it
with the Vision Pro,
but a lot of the vendors
did begin to realize
that spatial probably was
the good, a good term to use
to identify the whole area
of AR, XR, VR, MR, et cetera.
My name is Tim Bajarin.
I'm chairman of Creative Strategies.
I've been in the industry for many years.
This is actually my 50th CES.
Yeah, I actually probably need more
of my head examined than applause,
but I've been involved
with CES for a long time
as an advisor in working with them
and especially working
on the conferences over the
last 20 or 30 years or so.
This is a very important show.
It's one that centers on
not only the technology
that's here, but what
the industry's working on
and where we are gonna go with it.
So today, our particular
panel, and I brought together
some really important
experts on the subject,
but what we're gonna explore
today is what's going on now.
And then it is kind of interesting.
In fact, if I can go
backwards, the actual title
was 2030, the Impact of XR Headsets, AR,
and Smart Glasses on Special Computing.
I did not put 2030 in my title,
that how somehow got on there.
I have learned as an
analyst who does predictions
that predicting 10 years
or even five years out is a foolish idea
because it's amazing
how much changes even in a year.
And I'm just looking back
from our panel last year
and how much different this panel is
and what we're gonna talk about
today compared to last year.
The bottom line is, I have
wonderful speakers today.
Kelly Ingham, VP of wearable
Research Partnerships
Reality Labs at Meta,
Jason McGuigan, is it McGuigan?
- [Jason] McGuigan.
- McGuigan.
Okay, I never get that right. Sorry.
Head of Virtual Reality at Lenovo.
And Scott Myers, VP of Hardware at Snap.
Now, all of these folks have been involved
in this particular area
for quite some time.
As a researcher, I'm also a user.
I use my meta Wayfarer glasses
consistently, especially
for taking pictures, recording
notes for the AI capabilities
that have just been introduced
in the last year or so.
And Kelly's gonna give us a
little bit of an update on some
of the new software that's,
that's going on there.
I was early in one of the first actually
to see the Vision Pro
and got to understand a little bit about
what Apple was doing
from its earliest stages.
If I have time,
I'll comment on the Vision
Pro a little bit later.
But where we are today
is VR is going to be a
very interesting area
still from the standpoint of the headsets
and its impact, especially
in things like business
and enterprises, et cetera.
In that one context, I want to recommend
that the session after mine
is with my friend and
partner, Barton Goldenberg,
who is gonna have three
speakers very specifically
on the role of VR and XR
and even headsets, excuse me,
smart glasses in the
business enterprise world.
And he's got amazing panels.
He's got one of the leaders
of Siemens who's done the
big projects for them.
He's got a woman from Honda who is kind
of gonna blow your mind
because they're doing all
of their work and design
on the Apple Vision Pro.
And Dr. Harvey Castro,
sorry, who is a medical doctor
who's been on the forefront
of using AR and VR
in everything from surgery.
And he was 20 years as
an emergency doctor.
So I encourage you to stay for that.
Now Kelly, I'm gonna start with you
because number one, right now,
when you look at VR headset,
Quest is the King of the Hill.
Numbers-wise, you are
the number one headset.
So let's start just
with the Quest as it is
and its success.
And then I'm gonna go to Jason
and have you talk a
little bit about your VR.
So I'm gonna start out
with the VR side, okay?
- Sure, great.
Thanks Tim. I'm really excited to be here.
I hope everybody's awake this morning.
You know, we're opening the show here.
Talk a little bit about Quest.
It's been an amazing device.
We've launched our Quest 3
with digital pass through.
I hope everyone has had a
chance to play with that device
and experience the pass through.
We had the Quest 3S that
we also launched this year.
Some of the things that we're
working on is continuing
to drive into our pass through
and improving that technology.
We're also looking to make things lighter
and with a better form factor.
We want people to spend more
time in their Quest devices.
I recently met a woman who is
a population one power user
and she has a battery pack
that she straps to her head so
that she can play for five
hours at a time, right?
So we need to get to a point where people
that wanna play and be in device
for five hours at a time don't have
to add an extra battery pack.
And so we are working on those areas.
And so form factor is gonna be a big space
that you're gonna see
more advancements from us
in the VR headsets.
We're also looking at
bringing more users to play
with the Quest 3S and the price point.
We brought a lot of younger audience
and younger gamers, they're
really interested in that device
and they're consuming a lot
of our freemium content.
Another area is we're focusing
in the content space on 3D.
I think many of you saw
that we have a partnership
with James Cameron.
He's one of the most
innovative storytellers in 3D.
And we're gonna be bringing a
lot more 3D content to Quest,
which I'm really excited about.
We're also seeing new use cases.
People love to multitask
with their Quest devices.
There is a YouTube series now
where people are putting on
their Quest and then going
and vacuuming, doing the dishes, right,
and doing other types of
interactions around the house.
And so what this is showing is
that people really enjoy having
a virtual screen with them
to do different activities
and they're leveraging the
digital pass through to do that.
And so you're gonna see
us leaning into more areas
where people can multitask
and enjoy the great content
they're used to live,
you know, NBA games and
gaming content as well
as the additional 3D content.
So those are some of the big
things that we're gonna be
working on, on the Quest line.
- Jason, I've known Lenovo's work,
especially in the enterprise,
which has been kind
of your lead category that you focused on,
especially over the
last four or five years.
Update us on where Lenovo is on this,
'cause I know you're making a little bit
of a transition also too,
including something we'll
talk about in a few minutes,
smart glasses, but what's your experience
so far in using XR headsets
or VR headsets in enterprise
and where do you see that going?
- Yeah, that's a great
question. Thanks, Tim.
So at Lenovo we do focus very
heavily on the enterprise
space within XR, you know,
but you know, I think as you mentioned,
there's a great deal of interest
of how we're actually transitioning
from what the consumer market's doing
to what the enterprise space
is actually doing at Lenovo.
When we're, you know, figuring out
our next agenda, our next
path, it's very much led by
how people actually utilize the device
sometimes in the consumer space.
Now Kelly mentioned people
multitasking in the device.
When you look at it from
a enterprise perspective,
we don't necessarily want a
worker going into the office
and then doing their work
and then jumping on Netflix
or going into a gaming platform, right?
So there is a bit of a
difference in the market,
but it does come down to
how we keep people in devices longer.
And that for us goes
down to device utility.
So how often people will be
able to stay in the device
and do work related tasks
and jump to other pieces
of spatial content
that also have work
related tasks associated.
But when we really look
at that overall spectrum,
where we focus our energy both previously
and now is around the solution approach.
It's not necessarily
just about the hardware.
And for us, we've focused on
hardware, we have great pieces
of hardware we've
developed, we have partners
that work in the hardware space,
but at the end of the day,
it's what people actually can do with it.
And when a business makes a
decision to bring devices,
XR devices into their
space, they are doing it
with a business use case.
They're doing it for a purpose of ROI,
not to give somebody
something entertaining to use,
but to actually make a
difference for their platform,
for their organization.
So when they start looking
at it from that perspective,
they say, all right, well,
I'm gonna bring this into my office
and I'm gonna bring this into
my whole entire organization.
How do I make this worthwhile
for us to spend the money
to do the development
and to bring this up?
So when we look at it
from that perspective,
the last thing they're
usually talking about is
what are the specs on the device?
They're talking about how
they're actually gonna make a
difference in their organization.
And that's, I think our area
that we really focus on is when we talk
to a customer about it, when
we talk to an organization,
we talk to the world about it,
we're focusing on the actual solution
and not just the product.
- Kelly, while we're on the whole role
of XR headsets in general, you obviously
and the Quest have had
great success in gaming
and the consumer, but I do know
that it's being used in some enterprise
applications as well.
Can you share about some
of the enterprise functions
or areas that it's gotten
some success as well?
- Sure, Tim.
I think some of the early
successes were in training.
You know, one of the big use cases,
Walmart actually brought this on
and actually put all of their shelves
and actually mocked up like
basically a Black Friday sale
in the Quest headset so
that their store associates
could know how to deal with the influx
of all the customers, all of the demands,
yelling, screaming, and
trying to grab devices.
And actually they were one
of the first early use cases
in using these headsets
to really train people
and do it much faster
and much more efficiently
using the headsets.
And so we see a lot of different
applications for training
and kind of simulation exercises.
Another area that's
been really interesting
is on the medical field, surgical VR
has been a great advancement
where they're actually
using these headsets
to actually show surgery
in different practices
and be able to get the surgeons
and the associates to actually learn how
to actually advance very complex
techniques without having
to use cadavers, which are
actually quite expensive.
And so that's been another, you know,
I think enterprise application.
Education is obviously an area
that they've been leaning into quite a bit
with the VR headsets.
And then we're partnering with companies
to actually enable us to go
even broader into enterprise
that can actually bring
those enterprise solutions
and tool sets like Lenovo to go
and really customize those
for those enterprise applications.
We're very much focused on
the consumer business, right?
And driving these headsets into consumer.
And so we've been really excited about
what our developers have been
able to do with the headsets
and actually using it for applications
and content that I think we couldn't even
think about on our own.
- Alright, we may come back
to the XR headsets in a minute,
but I wanna shift a little
bit over to the smart glasses.
This is an area that I've been looking at
for quite a while
besides this particular, the
Wayfarers I was in early-
- His Wayfarers are talking to him, now.
I can hear his music.
Just say, "Hey Meta, stop."
This shows connectivity
on the glasses right now.
- I am a Jimmy Buffet fan
and how, I can't tell
you how that happened.
Anyway, anyway,
anyway, bottom line is smart glasses.
Besides this, I used one called the X Reel
and it was one of the
first ones that came out
and I literally used it on
airplanes watching, you know,
my movies on that.
But Smart Glasses actually
debuted pretty early with Snap
and Scott, if remind me, I believe it was,
was it 2015 or 2019?
- We launched in 2016.
- 2016!
- Camera only. Smart Glasses.
- So you've been in this
smart glasses for a while.
Tell us a little bit about
the thinking about, you know,
getting into that market,
but more importantly
where you are today.
- Oh yeah, we've been
building smart glasses
for a really long time
and while they're fun
and we think there's
like a near term market.
We're actually focused on the long term.
- Scott put you-
- If you ask people what they really want,
what they'll describe
to you in the long term
is see-through immersive AR glasses
and we've actually built them
with our spectacles product,
which is available today.
So, you know, we think that they're...
Big, a lot of potential here
as we're building use cases
and SNAP is already a scaled
augmented reality player.
I think that's really
important for people to know.
We have 300 million people using
augmented reality every day.
375,000 developers have
built 4 million lenses
and the use case is really simple,
it's just about having fun
and that's what we
designed spectacles to do,
both inside and outdoors.
We've made spectacles work
indoors, it's like really hard
to make them work that way when we have
like see-through immersive
AR and indoor capability.
But making them work
outdoors is far harder
and we've done that and
you can do it today.
It transitions from clear to
tinted in the blink of an eye
and it's super fun and we
think that's the future
for the long term.
- Okay.
Now Kelly, you most
recently made quite a bit
of splash at your developer's conference
with the introduction of the Orion.
Tell, first of all,
some people may not have
seen that announcement,
but tell us a little
bit about what Orion is
and kind of the timing,
as I understand it.
- Sure, Tim. So Orion is our AR glasses.
These are fully immersive glasses
that we showed at our
Connect developer conference.
This is really showing
a vision of the future.
They're not commercial glasses today.
And these glasses really
bring together all
of the technology and research
that we've been working on
for the past 10 years in research labs.
I've been a part of that for
about seven and a half years.
And the glasses are really
focused on a form factor,
highly immersive experience.
We've been very open that
we're looking at displays,
we're using silicon carbide in
these glasses for wide field
of view and high resolution.
We're also brought in eye tracking
and some of our new eye
tracking technologies.
We have custom silicon to
do both compute as well
as enable battery life.
We have our Meta AI assistant
on the glasses as well.
It does come with a separate compute puck.
We're also integrated our band technology.
So for many of you we have our EMG band,
which allows us to
really understand inputs
and interaction devices in a seamless way.
And so the combination of
those experiences of being able
to have input via your
voice, via your eyes
and via the band.
And then this highly immersive experience
where you can actually
put up three screens
where you can actually scroll your feed,
your Instagram feed, you can
actually do a voice conference
at the same time or you can
actually see photos or pictures
or maybe you're working, you
know, something with your,
something with Lenovo at the same time.
But you can do all of
those on this wide screen
and then be able to navigate the room
that we have in the demo.
And this is really showing
what we see the promise
of spatial computing and
the promise of glasses
because this is in a glasses form factor.
Where we're going with
this is we do have plans
to commercialize this device.
So gonna be working over that
on the next couple of years
for fully immersive AR displays.
We have a lot of work going on
in research right now looking
at displays from small field of view up
to large field of view.
And I think you're gonna see some of
that technology come
forward in our smart glasses
and our overall glasses roadmap.
- And timing-wise,
last I heard it was about
a three year timing, like,
but is that still kind of where we are?
- That is still where we are.
I'm sure we are trying
to accelerate as much as possible.
- Okay, good.
Now Jason, when we talk
about XR headsets in general,
and even smart glasses,
Google made an announcement
around what's called Android,
is it Android X, I believe.
- Android XR.
- XR, XR.
I know you guys have partnered
with Google on various.
And can you tell our audience
a little bit about what XR is
and what do you think the value
of adding another operating
system to the XR scheme?
- Sure, yeah, no, it's a, it's a great
and important question
for the entire ecosystem.
XR is kind of the umbrella term
for everything we're working on.
So everything from a wearable device,
and you could even start to
include the wearable devices
like the Meta RayBan glasses
to fall starting into that realm.
I mean, you're not
quite extending reality,
you're not putting an
overview on anything just yet,
but we see where that's going.
Obviously these devices
here that, you know,
they're both wearing,
are that training wheels
to where we want to get to,
where we actually have that
pass through on at all times
or that visual overlay at all times.
But that umbrella term XR
or spatial compute,
I think we're all in
this room familiar with.
What Google announced, I
think was very important
for the overall ecosystem
because they're a major player
and they have a platform.
Now, platforms are extremely
important to who, you know,
to the development community primarily
as well as the customers.
And when a developer looks
at where they're going
to actually create their
product for, they want to go
to a platform that has
customers already on it,
they're small companies,
they're investing a lot
of their energy into it.
Of course they wanna make sure
that when they invest into it,
that it goes onto a platform
that they have an ease of distribution.
So Google as a player in the
space that has the play store
and has the ability to
massively distribute content to
third parties and to vendors
that will allow them to
essentially accelerate
that portion of the market
and step into a role
that only meta previously
has had purview into.
So I think it does create a little bit of
that competition aspect.
It does allow for more devices,
more players to come to market.
And the expectation is, is
that that will allow other OEMs
to develop devices on their platform.
Or as meta announced with Lenovo
and some other third
parties, the Horizon platform
to be able to develop a open ecosystem
to allow multiple OEMs to
create devices for those,
you know, more umbrella platforms.
So that way when a
developer comes in they say,
do I wanna develop for
the Meta Horizons platform
and go towards these devices
or do I want to develop for
the Android XR platform,
develop for these devices?
Now hopefully what we get to,
and we've spoken about this,
the three of us earlier
spoke about this, is we want
to get to a world where,
of course it doesn't matter
where you're developing for
that your content can work within it.
Similar to how the web is today.
You know, we developed
something that ends up
being on a web browser.
It doesn't matter if you look
at it on your phone on a Mac,
on a pc, wherever you are,
you still get a reflection
of that content.
It's, you know, it's of course
not in the immersive space,
A little bit more easy to tailor,
but thanks to the powers of generative AI,
we're gonna see a lot of
that uptick where we're going
to create once and it's going
to be able to be folded over
to any platform.
And we're gonna see that in
the very, very near future
where the vision of OpenXR
will start to come to fruition.
We're not gonna be stuck in
this world where we develop once
and then we have to
spend eons developing for
another headset that just came
out and now changes the game.
We're getting to that near-term point
where we are going to see XR become
a little bit more unified.
And I think that announcement really
does show that that's where we're going.
- So what you're announcing is Nirvana XR.
- I mean the hope is of course that again,
a developer with, you know,
a small team can create something
and it can go on to wherever
the customer lands, right?
And if the customer says,
I've got these devices,
I want this piece of software
to work on that, great.
That's where it goes.
When Adobe, for example, sells a piece
of software, they don't tell
you what computer to buy to run
that piece of software.
That is up to the end customer.
So I think that's where the
world wants, you know, XR to be,
is into a platform space
where a developer can create something
and it can be consumed
on whichever XR device
that person ends up having.
And the XR device will work
within its own constraints
to create the experience related
to that piece of content.
So if it's just, you
know, an AI-powered glass,
maybe it's a truncated experience.
If it has a visual overlay
and it's more of an AR
glass, it has one experience
or a more advanced experience
and then if you want
something fully immersive,
it could be more of a mixed reality
or virtual reality device.
- Scott, I want you to weigh
in on this a little bit
because you guys have
spent quite a bit of time
and a lot of energy developing
your own operating system,
which has been very successful.
What Jason just mentioned
is a really important thing
in the context of getting
software developers
and you gave us a number
on developers that you have
for your operating system now.
Tell us, give us a little more of a sense
of your thinking about that
growing your operating system
and the developer network
and how is that going to be able to
continue to grow in that context?
- Sure, so we built SNAP OS
because we had to, there's really nothing
else in the world like it.
And there's a couple reasons why.
We have Snap's spatial engine,
which not only uses six degree VIO
to put content in the
world and keep it there
as you move around,
it's actually powerful enough
to augment dynamic moving objects
like a basketball in the scene.
This required intense
focus on both the camera
and the rendering pipeline
to drive down the latency
to the point where that's possible.
And it's amazing.
We also have an operating
system built on hands and voice.
You have everything you need
to interact with spectacles,
you don't need any controllers.
And making hand tracking
work on large glasses
or large goggles is pretty hard,
but making it work on
glasses is far harder.
Not only does it need to
be as accurate, it needs
to be way more power efficient.
One of the amazing things
about spectacles is
that it's connected.
So if we're all in the same room
and we have AR content out in
front of us, we can interact
with it together as if it was real.
And that's built deep
into the operating system
that connected nature.
And finally, spectacles
is completely standalone.
You don't need a puck, it's
not tethered to a phone,
it's all just crammed into the device
and that creates some
intense thermal challenges.
And so we have a unique dual system
on chip architecture using
Qualcomm Snapdragon processors
where just like your brain has two halves,
we separate out the processing
and it allows us to spread
the heat around much better.
And we think it's really unique
and it actually enables the
kind of experiences we want
to build because that's
where we think the value is,
is having fun with people.
- Now I've been studying
this area for quite a while
and one of the things that has been a,
what I would call a roadblock
and I want you guys to
discuss, to weigh in on this,
is really the lenses and the
ability to take the content
and display it.
I've seen a couple, I actually,
I saw three yesterday where,
you know, the information
is in front of you
is in the glass lenses, I should say.
In the Lenovo, you know,
I've been involved with
Lenovo for decades.
We, early on, there were
these really key discussions
of the role of next generation lenses
that were gonna be
necessary to deliver the AR.
Can you tell us about not only what kind
of lenses you think are
gonna have to be there,
including in the Orion for example,
and what are the roadblocks to get there?
Are are we seeing breakthroughs, you know,
with wave guides and you know,
getting to that point now
or where are we at this point?
Because what I'm trying to figure out,
and I gotta tell you, I've tested now,
I don't know how many smart glasses
and headsets, pretty much
every one you can imagine
that are commercial.
And I agree with you
that I eventually want
the Nirvana experience.
I'd like to be able to take
whatever that content is
and use that on any device.
Getting there is not that easy.
I think that this, we as an industry
are gonna have a technical
challenge to get to that point.
And I don't know when we,
when and if we ever
get to that Nirvana XR,
but that should be our goal, right?
- Yeah, so I think we'll get there,
but I think the challenge
obviously is, you know,
as you mentioned the
technological barriers,
but it comes down to
what Scott was mentioning
around like things like heat
or then of course the cost.
And you can make these devices today,
but they're ridiculously
expensive to create.
If you're gonna make a
one-off as a prototype, great,
you can get a lot of
this experience in there,
but how do we get down
to a commercializable,
scalable fashion?
And then how do we make it
wearable for all day with,
you know, battery life
with, like heat dispersion,
all of the challenges associated with it.
Now the lenses are a part of the equation
and the optics are absolutely part of it.
I mean, but you can get
amazingly crisp visuals
in a small area or you
can get it more spread out
and less crisp.
Getting the two together, wide FOV,
lower power consumption,
all those aspects really takes
a lot of engineering feats.
But I think where the, you know, nirvana
of all this is going is that
if you do have that need
to get into that more
fully immersive experience,
you go to a different device
and I think we'll see an
ecosystem of device is not
that one catchall device
that kind of solves for everything.
- Scott?
- Yeah, so Snap, we have our
own custom optical engine.
So it not only has an
incredibly tiny liquid crystal
on silicon projector,
which has these super sharp, super vivid,
very bright images.
We have our own custom waveguides
and the rate of waveguide
development has blown me away.
We bought a waveguide
company a couple years ago
and I had a projection on
how I thought it would go
and we've far exceeded it
and this is all built on glass,
which is ready to scale today.
And so I think what we're
gonna see over the coming years
is wider and wider, more
vivid, more detailed waveguides
that allow us
to really combine the light
from the real world with
the digital light, but
also in a way that's clear.
So like you can see my eyes.
I think that's so important
because as people communicate,
you not only communicate
with your voice, your body language,
but also with your eyes.
I think that's gonna be another really
important aspect of it.
- Now Kelly, in the Orion design
and my son was at the event, I watched it,
but it's the same
scenario where I know that
the visuals are in the
glasses themselves, right?
Tell us a little bit about where,
because, you said
potentially three years out,
what are the things that are
keeping you from doing it now?
- Yeah, I think just to feed on both Scott
and Jason, like the optic
systems in these glasses
are extremely difficult.
Like this is one of the big challenges
that we have to solve.
And I see a lot of people
nodding their heads here
because it's not just a
materials question, right?
It's also a grading
and how do you actually put,
how do you actually get the
optical engine to align,
you know, with that wave guide?
And then you also need to look at,
make sure that you manage
that is socially acceptable.
You know, Scott said like,
you have to be able to see your eyes.
Well not only that, your
eyes can't glow, right?
So you have to manage that
light leakage on the front side
because you don't wanna
look like a robot talking
to people when you have your screens on.
And managing light leakage
is not an easy thing to do
for anybody that's kind of nodding
because there's other
trade offs in the system.
Second is, glasses are lightweight.
The RayBan Metas I'm wearing
today are 48 grams, right?
Even Orion right now is
less than a hundred grams
that still feels heavy over
a longer period of time.
And when lenses right now are
about two millimeters thick,
if you add a wave guide,
they're four millimeters thick.
So are you gonna have one
fat lens or two fat lenses?
And so when you start really
thinking about form factor
and really getting to
glasses and acceptability,
it even constrains the space
you're working with even more.
And so when we think about
what constrains us on Orion
is we're trying to do all
of this, right, with highly
immersive wide field of view
with high resolution with no light leakage
and a lightweight form factor.
And so a lot of the technologies
that we're using right now
aren't commercially viable
in the sense of they're high cost, right?
And they're expensive because
there's also this trade off as
how much is the user gonna pay
for this AR experience, right?
Are they gonna pay $10,000
per pair of glasses?
Probably not, right?
I mean if they say Tiffany on 'em maybe,
but, you know, so I think what
we're looking at is we need
to get to a price point
where we think the value
of these glasses are enough
that people are willing to pay
for them and that the
technologies that we put
inside can deliver the experience
that we think are fitting
for truly AR-immersive display.
And so we've learned a lot
from smart glasses, right?
Both on the consumers, how
to bring these to consumers.
You know what the things
we don't talk about
is everyone in this room
is really familiar with
consumer electronics, right?
Jo-Bob is not, right?
And so training people to
actually think about glasses
with technology inside
has been a challenge.
We spent a lot of time
doing demonstrations, right?
To teach people how do you use the
camera, how do you use the audio?
People are still blown away today
that there's audio in their glasses.
And so what we're trying
to also look at is we're
actually going towards that,
you know, user out there that
knows a lot about glasses,
that's the early adopter,
but they still have a price
point that we have to manage
and wait and experience and
have all those pieces together.
And so I think that's part of the delay.
Obviously there's silicon that you know,
is that we're working on right now.
There's battery, there's eye tracking,
there's the graphics pipeline, like all
of this has to come together.
And so being able to show it
and demo it in the Orion form factor
I think was a big breakthrough.
Now the next thing is coming
back to supply chain ops
and the rest of our teams of like make
this happen within a budget.
And I think that's what's really gonna,
the focus is gonna be in
how we bring this to life.
- You know, I mentioned that in the key,
in the title, it uses the word 2030.
As we talk, maybe the guys
from CES knew something,
oh, is 2030 kinda like
the timeframe that we're gonna
solve some of these things?
- Tim, everything in XR
is always five years away.
That's the simple-
It's the easiest role, right?
- I think we see a clear path
to broad consumer adoption
of AR glasses within this decade.
And lemme tell you how we're gonna do it.
We've already shipped real
AR glasses to real people
that are available today.
And we already have a scaled use case.
We know what people want to do with AR
and we see that AR glasses
is the next logical step,
but like I mentioned,
building them inside is hard,
building them outside is even harder.
So we're not asking people to do anything
they're not already doing.
Three of us on this panel
already wear glasses.
We're gonna just make those glasses more
and more powerful in a way
that lets the technology fade
to the background so people
can focus on living their lives
and having fun in a way that's
more natural, more human
and just more interactive.
- Yeah, I think that's the key, Scott,
is as we're seeing this
technology continue to develop,
I mentioned this training wheels idea
before, you know,
putting on a pair of the,
the meta Wayfarers, I've got a pair,
I use 'em all the time now.
When I put on my regular sunglasses,
I feel like I'm missing something.
Like there's a disconnect.
I get used to having that
extra bit of ability around it.
You throw on a visual display,
and this is actually a funny anecdote.
I have a friend of mine
that has tried out a bunch
of VR devices because I've
forced 'em to over the years
and he bought a pair of the meta glasses
and he's like, you know
what would be great?
It's like if they could have
like a little display on it.
I'm like, well you mean like XR devices?
He's like, oh, is that what
you guys have been working on?
I'm like, yeah, that's what
everybody's been working on
this is the whole idea, right?
So this is the graduation
of the tech, right?
And as we see more development go into it
and we see the ability for
these, the wave guides to be able
to get smaller, lighter,
faster, better, that graduation
of the technology, it'll start
to get slowly implemented.
But I think we are still gonna
have some of the challenges
and some of the reasons why
some folks are offloading power
processing and things like that
to a separate battery device,
puck, whatever it may be.
You know, I think that
we're going to, we see this,
this crossover where some
people say, you know what?
I need to have it all in one
or other folks are gonna
say, you know what?
I wanna have a companion
device that's gonna run
and I can wear this
device maybe a little bit
longer 'cause it's a little bit lighter.
Whatever factor rolls into
it absolutely has to go down
to the wearability and the
individual user saying,
I'm comfortable using
this in my daily life.
- And let's just add even
more to the equation.
Most people wear glasses today
because they need rx, right?
And so like if you look
at sun form factor,
those are easy users to go after.
I have clear these are
transition lenses on
right now, they're great.
We actually see with our RayBan metas
that we have the higher percentage
of transition lens users
because people wanna go indoors
and outdoors with the
same set of smart glasses.
But the big population here
that wears glasses is rx.
So you need to add RX
into this display equation
that we just talked about, right?
Which is already heavy,
which already you have the
social form factor issues
and then you're gonna have
to revolutionize the supply chain.
And this is one of the reasons
that we partnered with Esil
or Luxottica is that we feel that is one
of the big challenges that
we're going to have to solve.
And they're one of the best partners
that we can actually
solve that with as well
as they have the distribution
that gets us into the consumer
that teaches that consumer
both on the optical
and the retail side as well
as it helps us leverage their brands,
which people then have trust
that hey, this is gonna look
and feel like a RayBan product.
- But I do think it's worth it to get
the displays into the glasses themselves
because it just makes it
so much easier for people
to understand what to do.
One of my favorite parts of working
on at this job is getting to do demos
to skeptical very serious people.
And we get them laughing and smiling
as they're pinching AR bubbles moving
around augmented reality
jellyfish with our tutorial
and then they're just off to the races,
playing golf, building Legos,
usually with very little
instruction from me.
And then with the younger
generation, they love
to like imagine things.
So we have a lens called Imagine Together
where you literally say what
you are thinking, it pops up
as a 2D image.
We use gen AI to build it
into 3D and it's interactable.
You can move it around, you can play it.
And then with connected lenses
we can play with it together.
So I was playing with
my daughter, she wanted
to have a tea party, we,
she imagined a teacup
and all the stuff that goes
along with a tea party,
we put it on the table and
we just started playing
and it was so much fun.
And like that's what we're going for.
That's just that fun aspect
and it's amazing to see.
- Yeah, and I think we, you know,
we see the crossover with, you know, fun
and then we look at it, you know,
it's sometimes a little bit more of
the stodgier portion of the
enterprise space, right?
So how do we actually make this
into a business-usable tool, right?
And you know, when we start
looking at it from who's going
to invest in some of
these early technologies,
sometimes it does go to the
enterprise first and you know,
but you know, I think
we've talked about this
ad nauseum at this point, you know,
a lot of people will become to expect
that these devices will
be part of their ability
to bring into their jobs.
You know, the privacy aspects of it.
All the challenges associated
with these devices,
but there's going to be
so much of a consumer use
of this technology that there's
going to be an expectation
that you're gonna be able to use it
on the job as well.
I think, you know, many of us started off
with our own personal phones, you know,
cell phones being able to use them
and then you start bringing 'em into work
and it had to be this graduation
of allowing organizations
to allow for your
personal phone to be used
for work related needs.
And we're gonna see the same level of kind
of graduation over from fun into work.
But all of those things are
absolutely gonna work together.
There is one thing I think
that you both talked about is
just the interaction points.
And you know, the ability to have
a device where you just put it on,
and can interact with a virtual world
or interact with virtual
objects is really key.
And, you know, I want to call out
what meta is doing on, on Orion
as something I think is
going to be a a new industry
altogether is that band.
Now that band seems simplistic,
but for anybody that's worked in XR
and has used hand tracking before,
one of the biggest drawbacks of that is
a lack of haptic response.
And getting even just a
little bit of vibration,
a little bit of tactile response
when you're manipulating
something is so much more of
a connection to that object.
And I think we'll see that with,
you know, across the ecosystem.
I'm sure you guys are working
in that direction as well,
that there's a need for people to say,
oh I actually have
physically engaged with that
and it's gonna cement that
interaction even further.
It's gonna make it feel
more tangible to them.
It's gonna make it feel more real.
And I think you guys did
a great job with that
and I think I'd love
to see that become more
of the norm throughout XR
and interaction is that we
need something that triggers
that mind link that I did
actually touch that thing.
'Cause otherwise you're just,
you know, interacting with
that little ghost out there, right?
So when you reach out
and you pinch, if you could just feel that
makes all the world difference.
- Well, and I think one of the things
that we've been really focused on is
that people need multimodal,
they need different ways
to be able to interact their glasses.
So we have voice, which
I won't say "Hey Meta,"
'cause my glasses will go off,
but you have voice, right?
We also, we have the touch, right?
So you can take a photo,
having the band gives you
discreet input, right?
You could actually be engaging
with your meta AI
assistant very discreetly
by just doing clicks.
And to your point, like having
that haptic response gives you confidence
that it's actually hearing
and responding to you
in discrete function.
And so I find right
now, today I spend a lot
of time clicking my glasses,
especially if I'm taking action videos
of me skiing down the slopes
because you know, the voice,
the wind noise, right?
It won't pick up my voice
with that wind noise.
And so I think the band is
gonna open up a whole new set
of interactions, both
just on smart glasses
and AI assistant, but
also as you go to displays
and being able to navigate those displays.
- Just to add to that,
like the way we actually designed Snap OS
is the interface is literally
in the palm of your hand.
So you get that physical
touch as you touch your finger
and it feels like you're
touching the button.
- Yeah, just it is a really
cool feature on the Snap.
I'd be remiss to not ask this question
and unfortunately I'm doing
it with like a minute left,
which is bad.
AI on the meta,
Wayfarers was really kind
of a breakthrough for me.
The fact that you could use
your headset or use that
and you're seeing more and
more earbuds, et cetera,
adding the AI feature.
The other thing that you
and I were talking about is translation.
I've tried two or three glasses
and or earbuds that do translation.
I think those two are
kinda underappreciated as a
part of this next generation
of applications.
The idea of being able
to have those headsets
and I saw one from Evan Labs last night
where the president was
speaking to me in Chinese
and what he said was
showing up in the glasses.
And that was a game changer in the sense.
And then of course being able to just say,
"Hey, meta," you know, "what time is it?"
Or where am I going?
Actually, I don't ask it that
'cause you'll never know,
but you know what I mean?
The role of AI in spatial, I think,
is gonna be very important.
And I want to, if I can
get this screen up on this
and by just telling you
a little bit about a site
that my partner and I
have created called AI
and Spatial Computing Resource Center.
That's the connection to
it. This is a labor of love.
There's no cost.
We have, Barton and I have
written hundreds of articles
and pieces of material
on this particular area,
and we brought in a lot of
material and case studies
and this is a good resource center
because the AI connection is still new.
I mean, you know, all, you know, you can't
by the way go anywhere here
and any booth without AI being brought up.
So we kind of determined
that it's a trend.
The bottom line is we felt
that this started out as,
as was called the
Metaverse Resource Center,
but most recently we
started bringing in more
and more of the AI and spatial
computing content to this.
So if you can get a picture of that
symbol there, it'll take
you right to the guide.
We're unfortunately right out of time.
But I do want to each ask
each of you, you know,
to weigh in on just one last thing.
XR and AR have kind of solidified
where they are today.
Gaming, education, training, enterprise,
and there may be some.
And if you're in the next session,
Dr. Castro will bring up some
amazing examples in the area
of it use using in surgery.
But if you actually look
at the one big theme from this show,
you're gonna walk away from
it saying that Smart glasses
was a really important new
thing that was brought up.
Where are we going to be
next year with smart glasses?
Are we gonna see breakthroughs?
Is this gonna be more
of a developing year?
2026? Is that the next year?
Yeah, 20, I've totally lost years.
I'm getting so old that I don't even
know what they are anymore.
Anyway, 2026, what do you
think is gonna happen?
- Well, from a meta point of view,
our RayBan metas are
selling very well, right?
We are leaning in to continue
to drive growth on these glasses.
I think the biggest
learnings we have is really
understanding, you know,
and demoing the glasses to the consumer.
You know, we have to remember that to get
to broad consumer adoption,
this is a new technology,
this is a new use case in how
they're using the glasses.
We see that we are on the hockey stick.
I think the evidence of all
the new AI glasses you're gonna
see at the show today kind of shows that
we're gonna continue to
lean in that form factor
and style is important.
Two is we're continuing to lean
into our AI on the glasses.
We've launched many new kind
of AI commands, we had "Be My Eyes,"
which is one of the partnerships that were
for the low and vision impaired,
where they can actually turn on the camera
and say, look at this, right?
And then the user on the other side,
on the Be My Eyes
database can actually see
what they're seeing, and then
in their ear can actually give
them instructions on how to navigate.
This was actually a
really big breakthrough
for that community.
We're leaning in more
to, you know, reminders.
Like, hey, look at this.
Remind me later, where's
where did I park my car?
That's a big one that I use a lot.
And in our early access program
right now, we have live AI.
So you can actually live from
your glasses, actually show
what you're seeing and doing at that time,
and then put it on your
Facebook family of apps
and then live translation,
which is really a game
changer today in these glasses
so that you can speak with
someone in a different language
and then have it translated in your ear.
You're gonna see a cadence of more
and more launches of really
specific AI use cases
that help your assistant be
even more useful to you so
that you don't want to take
your glasses off, right?
At the same time, the
glasses have to look good.
They have to feel good, right?
And we need more and more consumers
to find out about the glasses
and to continue to kind of lean into them.
And so we're really excited
in terms of this growth year.
Some of the other things you're seeing,
we've been leaning into more styles.
You know, we recently
launched our headliner
and our Skylar style for
more of the female audience.
We're also leaning into more colors
and you're also gonna see
more limited edition launches,
like the one that I'm wearing so
that we target very specific audiences.
And so look out for this
space, I wish I could say more,
but we have a really exciting fall lineup
on the glasses form factor
that we're gonna continue
to be leaning into.
- Scott, a year from now. Snap.
- So I don't wanna comment on
specific roadmap necessarily,
but like we know now that
the hardware, software
and the authoring tools are ready to go.
So we expect to see this
explosion of use cases
focused on just helping
people live their lives better
and looking up not down.
And if anybody wants to see spectacles
after you let me know,
I'm happy to stick around.
- Okay, Jason?
You're gonna do some
partnerships this year, right?
- Yeah, so obviously we
did a big announcement
with Meta probably about
six months ago or so.
We'll be working with them
on the Horizon ecosystem.
So we will be leaning into
that relationship in this coming year.
So we're very excited to
continue to develop that.
But I think 2025 is going to be the year
where these devices become
socially acceptable,
where people will begin
to fully understand
that this is a thing that they're going
to see on a regular basis.
You know, whether it's a Snap like device,
a Wayfarer device, you know
the RayBan glasses style
or eventually an Orion type glass.
We're gradually building the community
and gradually building
the audience to expect
that a glasses form factor
with cameras on them,
with potentially displays could be part
of the interaction you're
having with another human being.
So when you see somebody
wearing glasses, they,
it's not going to be just
the normal expectation
that they're just a pair of eyeglasses.
You may be, you know, asked to,
you know, asked a question, all right,
well what's actually going
on behind the scenes there?
Are they seeing something?
Are they hearing something?
You know?
So that interaction point
becomes a lot more socially acceptable.
I think by 2026, the big
thing will be is that
we're gonna have an expectation
that these will be part
of our daily lives.
I think this will be the last year
where people will walk away
and say, you know what?
I don't have that need to be able
to speak to an AI assistant.
I don't have that, again,
the Chat GPT interaction
or the LM interaction
or whatever you're
actually using for your LM,
any of those tools
become such a part of our daily lives
that as we're moving into
this next generation of tech,
there's going to be this
absolute expectation
that it's built into
that future technology.
So I think we're at the
end of the generation
where there were dumb glasses
and we're moving into that generation
where there's an expectation,
almost like the TVs we have today.
When you buy a TV, every
TV now is a smart TV.
There's no TV that is not
internet connected anymore.
And I think in the very
near future we're gonna have
that same thing with our
- Well panel, thank you
so much for joining me.
It's really been a pleasure
to have you with us.
Your information and your knowledge
and your roles at your
companies are important
to us in the industry.
Thank you for joining us today.
- [Panelists] Thank you.
- [Tim] And I encourage you to stay
for the next session,
which you'll enjoy too.
Thank you very much!